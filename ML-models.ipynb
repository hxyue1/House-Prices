{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will assess a variety of ML algorithms in their performance in predicting house prices. We'll be considering lasso and ridge regression, random forests, xgboost and maybe later light gbm. All models will be tuned via Bayesian Optimisation to minimise the average 5-fold cross validation score. Note that here specifically we are predicting log house prices and using the RMSE metric so our model evaluation will need to be tailored to that.\n",
    "\n",
    "First we'll import the data and clean it so that it can be used in a ML friendly format. We'll use a similar process to the one in the EDA notebook. Since exploratory data analysis has already been covered in the other notebook, we won't bother repeating it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import skew\n",
    "from bayes_opt import BayesianOptimization\n",
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train.csv')\n",
    "test = pd.read_csv('Data/test.csv')\n",
    "\n",
    "#Stripping SalePrice from the training data and combining with the test data\n",
    "SalePrice = np.log(train['SalePrice'])\n",
    "train = train.drop('SalePrice',axis=1)\n",
    "X = pd.concat([train,test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSSubClass is a categorical variable, so we convert it to a string \n",
    "X['MSSubClass'] = X['MSSubClass'].apply(str)\n",
    "X['LotFrontage'] = X['LotFrontage'].fillna(0)\n",
    "train['GarageYrBlt'] = train['GarageYrBlt'].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dealing with numerics\n",
    "numerics = X.select_dtypes(exclude='object')\n",
    "\n",
    "#Filling nas\n",
    "numerics = numerics.fillna(method='ffill')\n",
    "\n",
    "#log transformation of skewed variables\n",
    "skews = numerics.apply(lambda x:skew(x.dropna()))\n",
    "skewed = skews>0.75\n",
    "skewed_data = numerics[skewed.index[skewed]]\n",
    "skewed_feats = skewed_data.columns\n",
    "numerics = numerics.drop(skewed_feats, axis=1)\n",
    "log_transformed = np.log1p(skewed_data)\n",
    "numerics = pd.concat([numerics,log_transformed],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 304) (1459, 304)\n",
      "(1460, 305) (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "#Dealing with strings\n",
    "strings = X.select_dtypes(include='object')\n",
    "\n",
    "#Converting strings to dummies and joining with numerics\n",
    "dummies = pd.get_dummies(strings)\n",
    "X = pd.concat([numerics,dummies],axis=1)\n",
    "\n",
    "#Splitting into train and test data\n",
    "X_train = X.iloc[:train.shape[0],]\n",
    "X_test = X.iloc[train.shape[0]:,]\n",
    "train = pd.concat([SalePrice,X_train],axis=1)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that the initial data preprocessing has been done, we are now going to set up cross-validation and Bayesian optimisation procedures for each model. The reason why we use Bayesian Optimisation for hyperparameter tuning is because of the stochastic nature of our objective function. We are randomly sorting the data to better replicate the variability in the dgp when measuring the validation score. While a deterministic optimiser may work, it may not be the best solution if it does not account for the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up optimisation for Ridge regression model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_score(alpha):\n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "    \n",
    "    scores = cross_val_score(estimator=Ridge(alpha=alpha), X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_ridge = {'alpha': (0,20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the parameter bounds, I have chosen alpha to be between 0 and 20. Using a 0 value will be equivalent to estimation by OLS, any positive values will shrink the parameter estimates towards zero. I do not want to using an upper bound that is too high as it will waste computation power as in practice, optimal values for alpha are relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1329  \u001b[0m | \u001b[0m 63.23   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.1284  \u001b[0m | \u001b[95m 13.79   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.133   \u001b[0m | \u001b[0m 75.6    \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1352  \u001b[0m | \u001b[0m 95.52   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 155.2   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1407  \u001b[0m | \u001b[0m 200.0   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-1.658e+1\u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1297  \u001b[0m | \u001b[0m 11.57   \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m-0.1282  \u001b[0m | \u001b[95m 10.57   \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m-0.1257  \u001b[0m | \u001b[95m 13.01   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.1266  \u001b[0m | \u001b[0m 13.47   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1285  \u001b[0m | \u001b[0m 10.24   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1259  \u001b[0m | \u001b[0m 13.62   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1268  \u001b[0m | \u001b[0m 10.17   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1281  \u001b[0m | \u001b[0m 13.63   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.1302  \u001b[0m | \u001b[0m 10.16   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.1281  \u001b[0m | \u001b[0m 13.63   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1293  \u001b[0m | \u001b[0m 10.15   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.1313  \u001b[0m | \u001b[0m 11.1    \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1277  \u001b[0m | \u001b[0m 12.64   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1301  \u001b[0m | \u001b[0m 10.15   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1293  \u001b[0m | \u001b[0m 13.64   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1309  \u001b[0m | \u001b[0m 10.15   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1266  \u001b[0m | \u001b[0m 13.64   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1287  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.1276  \u001b[0m | \u001b[0m 13.64   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1292  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[95m 28      \u001b[0m | \u001b[95m-0.1254  \u001b[0m | \u001b[95m 13.64   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.129   \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.1262  \u001b[0m | \u001b[0m 13.64   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1302  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.1292  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1312  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1279  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.1279  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1284  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.1269  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.1278  \u001b[0m | \u001b[0m 10.87   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1266  \u001b[0m | \u001b[0m 12.81   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.1273  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.1281  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1269  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.1294  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.129   \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.1311  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.126   \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1298  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1286  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.1271  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.1308  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[95m 51      \u001b[0m | \u001b[95m-0.1254  \u001b[0m | \u001b[95m 10.13   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.1287  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.1296  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.1288  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.1265  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune ridge regression\n",
    "BO = BayesianOptimization(ridge_score, bounds_ridge)\n",
    "\n",
    "BO.probe({\"alpha\":10},lazy=False)\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal choice of alpha seems to be around 10-20. Though depending on how the optimiser progresses, it can sometimes get stuck near the upper bound. I had to use the probe method to ensure that the optimizer started at a good value, otherwise it can get stuck on the upperbound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up optimisation for Lasso model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def lasso_score(alpha):\n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "\n",
    "    scores = cross_val_score(estimator=Lasso(alpha=alpha, max_iter=10000), X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_lasso = {'alpha': (0.00001,20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adjusting the bounds for the lasso as 0 values for regularisation cause convergence problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.3077  \u001b[0m | \u001b[0m 10.5    \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.3068  \u001b[0m | \u001b[0m 11.0    \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.3131  \u001b[0m | \u001b[0m 16.5    \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.3041  \u001b[0m | \u001b[0m 4.121   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.3163  \u001b[0m | \u001b[0m 19.36   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1382  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.14    \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1407  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.137   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.137   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.1405  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1385  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1387  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1387  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1387  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.1358  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.139   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1356  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1394  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.1394  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1431  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.1431  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1343  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1343  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.1343  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1364  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.1364  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.1368  \u001b[0m | \u001b[0m 1.002e-0\u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1384  \u001b[0m | \u001b[0m 1.002e-0\u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.1357  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.1427  \u001b[0m | \u001b[0m 1.003e-0\u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1416  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.1363  \u001b[0m | \u001b[0m 1.003e-0\u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.1363  \u001b[0m | \u001b[0m 1.003e-0\u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.1363  \u001b[0m | \u001b[0m 1.003e-0\u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.1416  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1416  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1374  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.143   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.143   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.143   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.1397  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.1397  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.139   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.139   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune Lasso regression\n",
    "\n",
    "BO = BayesianOptimization(lasso_score, bounds_lasso)\n",
    "\n",
    "BO.probe({\"alpha\":0.001},lazy=False)\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm quite suspicious about the behaviour of the Bayes optimiser in this case, it cannot seem to explore other values than the lower bound. While I could change the lower bound, it will cause convergence issues and will require an increased number of iterations. I can handpick a value for alpha that does noticeably better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12852462107663287"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_score(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I'm not going to try delve into the workings of optimiser as I don't think it will be very fruitful. Instead I'm going to try using a different optimiser to see if we get better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.26752613266792663\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-213127.13829123])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 50\n",
       "      nit: 2\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.99999921])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(lasso_score, x0=1, bounds = ((0.00001,20),), tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also performs quite poorly. I think it gets thrown off by the randomness of the objective function. It does find a good optimum, but only if I put in an optimal starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.1276295819828786\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([89516.17478705])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 36\n",
       "      nit: 2\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.00099648])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(lasso_score, x0=0.001, bounds = ((0.00001,20),), tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ridge and lasso seem to perform quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating optimizer for a Random Forests regressor\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "\n",
    "def RF_score(n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features):\n",
    "    \n",
    "    #Contraining hyperparameters to be converted to integers (e.g. number of decision trees can't be continuous!)\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    min_samples_split = int(min_samples_split)\n",
    "    min_samples_leaf = int(min_samples_leaf)\n",
    "    max_features = int(max_features)\n",
    "    \n",
    "    assert type(n_estimators) == int\n",
    "    assert type(max_depth) == int\n",
    "    assert type(min_samples_split) == int\n",
    "    assert type(min_samples_leaf) == int\n",
    "    assert type(max_features) == int\n",
    "    \n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        estimator=RF(\n",
    "                    n_estimators=n_estimators, \n",
    "                    max_depth=max_depth, \n",
    "                    min_samples_split=min_samples_split,\n",
    "                    min_samples_leaf = min_samples_leaf,\n",
    "                    max_features = max_features),\n",
    "    X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_RF = {\n",
    "    'n_estimators': (10,300),\n",
    "    'max_depth': (1,100),\n",
    "    'min_samples_split': (2,200),\n",
    "    'min_samples_leaf': (1,200),\n",
    "    'max_features': (10,290)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | max_fe... | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.2005  \u001b[0m | \u001b[0m 14.66   \u001b[0m | \u001b[0m 229.0   \u001b[0m | \u001b[0m 78.17   \u001b[0m | \u001b[0m 157.9   \u001b[0m | \u001b[0m 202.8   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.2155  \u001b[0m | \u001b[0m 79.78   \u001b[0m | \u001b[0m 104.8   \u001b[0m | \u001b[0m 113.2   \u001b[0m | \u001b[0m 159.5   \u001b[0m | \u001b[0m 84.32   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.2111  \u001b[0m | \u001b[0m 37.85   \u001b[0m | \u001b[0m 78.14   \u001b[0m | \u001b[0m 99.9    \u001b[0m | \u001b[0m 22.95   \u001b[0m | \u001b[0m 240.3   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1922  \u001b[0m | \u001b[0m 58.83   \u001b[0m | \u001b[0m 184.9   \u001b[0m | \u001b[0m 64.19   \u001b[0m | \u001b[0m 73.26   \u001b[0m | \u001b[0m 279.0   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.1795  \u001b[0m | \u001b[0m 23.88   \u001b[0m | \u001b[0m 255.2   \u001b[0m | \u001b[0m 10.39   \u001b[0m | \u001b[0m 101.8   \u001b[0m | \u001b[0m 32.23   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.2634  \u001b[0m | \u001b[0m 91.74   \u001b[0m | \u001b[0m 289.3   \u001b[0m | \u001b[0m 187.9   \u001b[0m | \u001b[0m 10.53   \u001b[0m | \u001b[0m 14.95   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.2061  \u001b[0m | \u001b[0m 99.12   \u001b[0m | \u001b[0m 20.86   \u001b[0m | \u001b[0m 1.042   \u001b[0m | \u001b[0m 171.5   \u001b[0m | \u001b[0m 255.6   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1772  \u001b[0m | \u001b[0m 17.46   \u001b[0m | \u001b[0m 20.48   \u001b[0m | \u001b[0m 14.38   \u001b[0m | \u001b[0m 10.51   \u001b[0m | \u001b[0m 16.12   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.1526  \u001b[0m | \u001b[0m 97.98   \u001b[0m | \u001b[0m 283.9   \u001b[0m | \u001b[0m 7.401   \u001b[0m | \u001b[0m 26.44   \u001b[0m | \u001b[0m 298.3   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.2295  \u001b[0m | \u001b[0m 2.927   \u001b[0m | \u001b[0m 261.1   \u001b[0m | \u001b[0m 2.241   \u001b[0m | \u001b[0m 6.328   \u001b[0m | \u001b[0m 217.3   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.2611  \u001b[0m | \u001b[0m 98.19   \u001b[0m | \u001b[0m 282.1   \u001b[0m | \u001b[0m 167.2   \u001b[0m | \u001b[0m 196.6   \u001b[0m | \u001b[0m 297.1   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.2051  \u001b[0m | \u001b[0m 89.2    \u001b[0m | \u001b[0m 271.0   \u001b[0m | \u001b[0m 2.472   \u001b[0m | \u001b[0m 198.0   \u001b[0m | \u001b[0m 35.75   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1515  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 300.0   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1471  \u001b[0m | \u001b[0m 98.28   \u001b[0m | \u001b[0m 83.48   \u001b[0m | \u001b[0m 5.186   \u001b[0m | \u001b[0m 12.59   \u001b[0m | \u001b[0m 139.1   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1992  \u001b[0m | \u001b[0m 94.32   \u001b[0m | \u001b[0m 18.64   \u001b[0m | \u001b[0m 31.88   \u001b[0m | \u001b[0m 2.668   \u001b[0m | \u001b[0m 13.46   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.2757  \u001b[0m | \u001b[0m 1.929   \u001b[0m | \u001b[0m 91.96   \u001b[0m | \u001b[0m 1.208   \u001b[0m | \u001b[0m 195.8   \u001b[0m | \u001b[0m 12.68   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.2032  \u001b[0m | \u001b[0m 99.35   \u001b[0m | \u001b[0m 271.4   \u001b[0m | \u001b[0m 4.545   \u001b[0m | \u001b[0m 183.3   \u001b[0m | \u001b[0m 293.9   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1612  \u001b[0m | \u001b[0m 99.26   \u001b[0m | \u001b[0m 287.1   \u001b[0m | \u001b[0m 12.44   \u001b[0m | \u001b[0m 54.73   \u001b[0m | \u001b[0m 117.5   \u001b[0m |\n",
      "| \u001b[95m 19      \u001b[0m | \u001b[95m-0.1395  \u001b[0m | \u001b[95m 94.5    \u001b[0m | \u001b[95m 162.7   \u001b[0m | \u001b[95m 1.24    \u001b[0m | \u001b[95m 2.528   \u001b[0m | \u001b[95m 294.8   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1498  \u001b[0m | \u001b[0m 91.44   \u001b[0m | \u001b[0m 236.5   \u001b[0m | \u001b[0m 2.429   \u001b[0m | \u001b[0m 5.78    \u001b[0m | \u001b[0m 12.12   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1534  \u001b[0m | \u001b[0m 99.3    \u001b[0m | \u001b[0m 150.8   \u001b[0m | \u001b[0m 5.051   \u001b[0m | \u001b[0m 38.98   \u001b[0m | \u001b[0m 171.8   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1498  \u001b[0m | \u001b[0m 98.2    \u001b[0m | \u001b[0m 147.4   \u001b[0m | \u001b[0m 1.085   \u001b[0m | \u001b[0m 32.22   \u001b[0m | \u001b[0m 286.8   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.2395  \u001b[0m | \u001b[0m 3.84    \u001b[0m | \u001b[0m 10.5    \u001b[0m | \u001b[0m 6.872   \u001b[0m | \u001b[0m 5.543   \u001b[0m | \u001b[0m 281.0   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.2921  \u001b[0m | \u001b[0m 99.15   \u001b[0m | \u001b[0m 16.06   \u001b[0m | \u001b[0m 195.2   \u001b[0m | \u001b[0m 5.274   \u001b[0m | \u001b[0m 295.0   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1423  \u001b[0m | \u001b[0m 99.47   \u001b[0m | \u001b[0m 202.6   \u001b[0m | \u001b[0m 3.541   \u001b[0m | \u001b[0m 2.117   \u001b[0m | \u001b[0m 89.45   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.2845  \u001b[0m | \u001b[0m 1.684   \u001b[0m | \u001b[0m 272.0   \u001b[0m | \u001b[0m 183.6   \u001b[0m | \u001b[0m 195.5   \u001b[0m | \u001b[0m 16.14   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.2193  \u001b[0m | \u001b[0m 2.821   \u001b[0m | \u001b[0m 135.2   \u001b[0m | \u001b[0m 4.598   \u001b[0m | \u001b[0m 2.2     \u001b[0m | \u001b[0m 17.28   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.157   \u001b[0m | \u001b[0m 96.46   \u001b[0m | \u001b[0m 286.6   \u001b[0m | \u001b[0m 8.409   \u001b[0m | \u001b[0m 8.261   \u001b[0m | \u001b[0m 10.85   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.1411  \u001b[0m | \u001b[0m 99.7    \u001b[0m | \u001b[0m 206.8   \u001b[0m | \u001b[0m 1.397   \u001b[0m | \u001b[0m 6.412   \u001b[0m | \u001b[0m 244.5   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.1436  \u001b[0m | \u001b[0m 95.66   \u001b[0m | \u001b[0m 138.7   \u001b[0m | \u001b[0m 4.276   \u001b[0m | \u001b[0m 6.752   \u001b[0m | \u001b[0m 205.0   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1407  \u001b[0m | \u001b[0m 99.3    \u001b[0m | \u001b[0m 53.78   \u001b[0m | \u001b[0m 2.933   \u001b[0m | \u001b[0m 7.344   \u001b[0m | \u001b[0m 223.0   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.1421  \u001b[0m | \u001b[0m 99.37   \u001b[0m | \u001b[0m 157.7   \u001b[0m | \u001b[0m 4.222   \u001b[0m | \u001b[0m 3.667   \u001b[0m | \u001b[0m 208.5   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1584  \u001b[0m | \u001b[0m 99.05   \u001b[0m | \u001b[0m 187.5   \u001b[0m | \u001b[0m 2.135   \u001b[0m | \u001b[0m 54.41   \u001b[0m | \u001b[0m 162.5   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1592  \u001b[0m | \u001b[0m 96.06   \u001b[0m | \u001b[0m 11.56   \u001b[0m | \u001b[0m 1.012   \u001b[0m | \u001b[0m 11.93   \u001b[0m | \u001b[0m 112.8   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.1549  \u001b[0m | \u001b[0m 99.86   \u001b[0m | \u001b[0m 126.1   \u001b[0m | \u001b[0m 2.055   \u001b[0m | \u001b[0m 49.76   \u001b[0m | \u001b[0m 278.8   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1413  \u001b[0m | \u001b[0m 98.95   \u001b[0m | \u001b[0m 158.3   \u001b[0m | \u001b[0m 1.956   \u001b[0m | \u001b[0m 4.108   \u001b[0m | \u001b[0m 106.9   \u001b[0m |\n",
      "| \u001b[95m 37      \u001b[0m | \u001b[95m-0.1393  \u001b[0m | \u001b[95m 98.69   \u001b[0m | \u001b[95m 130.8   \u001b[0m | \u001b[95m 2.687   \u001b[0m | \u001b[95m 2.767   \u001b[0m | \u001b[95m 290.7   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.1532  \u001b[0m | \u001b[0m 99.92   \u001b[0m | \u001b[0m 231.4   \u001b[0m | \u001b[0m 12.29   \u001b[0m | \u001b[0m 3.909   \u001b[0m | \u001b[0m 197.5   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1462  \u001b[0m | \u001b[0m 96.94   \u001b[0m | \u001b[0m 37.51   \u001b[0m | \u001b[0m 3.919   \u001b[0m | \u001b[0m 5.01    \u001b[0m | \u001b[0m 220.2   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.1395  \u001b[0m | \u001b[0m 99.77   \u001b[0m | \u001b[0m 139.0   \u001b[0m | \u001b[0m 1.491   \u001b[0m | \u001b[0m 7.524   \u001b[0m | \u001b[0m 164.2   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.1414  \u001b[0m | \u001b[0m 97.17   \u001b[0m | \u001b[0m 133.3   \u001b[0m | \u001b[0m 4.833   \u001b[0m | \u001b[0m 2.729   \u001b[0m | \u001b[0m 262.7   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1443  \u001b[0m | \u001b[0m 99.33   \u001b[0m | \u001b[0m 157.7   \u001b[0m | \u001b[0m 4.175   \u001b[0m | \u001b[0m 2.832   \u001b[0m | \u001b[0m 227.1   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.1517  \u001b[0m | \u001b[0m 98.49   \u001b[0m | \u001b[0m 191.9   \u001b[0m | \u001b[0m 11.27   \u001b[0m | \u001b[0m 3.863   \u001b[0m | \u001b[0m 299.0   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.1443  \u001b[0m | \u001b[0m 99.35   \u001b[0m | \u001b[0m 164.0   \u001b[0m | \u001b[0m 5.372   \u001b[0m | \u001b[0m 4.497   \u001b[0m | \u001b[0m 167.8   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.1484  \u001b[0m | \u001b[0m 97.9    \u001b[0m | \u001b[0m 45.04   \u001b[0m | \u001b[0m 5.228   \u001b[0m | \u001b[0m 6.479   \u001b[0m | \u001b[0m 202.1   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.1415  \u001b[0m | \u001b[0m 98.76   \u001b[0m | \u001b[0m 128.4   \u001b[0m | \u001b[0m 3.062   \u001b[0m | \u001b[0m 3.808   \u001b[0m | \u001b[0m 243.1   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1551  \u001b[0m | \u001b[0m 99.37   \u001b[0m | \u001b[0m 214.3   \u001b[0m | \u001b[0m 1.391   \u001b[0m | \u001b[0m 40.51   \u001b[0m | \u001b[0m 25.99   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1403  \u001b[0m | \u001b[0m 98.23   \u001b[0m | \u001b[0m 123.7   \u001b[0m | \u001b[0m 3.993   \u001b[0m | \u001b[0m 3.976   \u001b[0m | \u001b[0m 294.0   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.1452  \u001b[0m | \u001b[0m 99.93   \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 1.124   \u001b[0m | \u001b[0m 17.51   \u001b[0m | \u001b[0m 191.5   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.144   \u001b[0m | \u001b[0m 99.5    \u001b[0m | \u001b[0m 277.2   \u001b[0m | \u001b[0m 1.009   \u001b[0m | \u001b[0m 12.28   \u001b[0m | \u001b[0m 111.1   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.1397  \u001b[0m | \u001b[0m 97.9    \u001b[0m | \u001b[0m 115.6   \u001b[0m | \u001b[0m 1.495   \u001b[0m | \u001b[0m 3.936   \u001b[0m | \u001b[0m 168.3   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.143   \u001b[0m | \u001b[0m 97.63   \u001b[0m | \u001b[0m 144.6   \u001b[0m | \u001b[0m 4.899   \u001b[0m | \u001b[0m 5.026   \u001b[0m | \u001b[0m 275.0   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.1424  \u001b[0m | \u001b[0m 99.71   \u001b[0m | \u001b[0m 209.5   \u001b[0m | \u001b[0m 1.592   \u001b[0m | \u001b[0m 10.27   \u001b[0m | \u001b[0m 297.6   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.146   \u001b[0m | \u001b[0m 99.61   \u001b[0m | \u001b[0m 192.9   \u001b[0m | \u001b[0m 5.105   \u001b[0m | \u001b[0m 4.608   \u001b[0m | \u001b[0m 233.5   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.1506  \u001b[0m | \u001b[0m 99.07   \u001b[0m | \u001b[0m 189.4   \u001b[0m | \u001b[0m 9.595   \u001b[0m | \u001b[0m 3.441   \u001b[0m | \u001b[0m 107.7   \u001b[0m |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune Rando Forests regression\n",
    "BO = BayesianOptimization(RF_score, bounds_RF)\n",
    "\n",
    "BO.probe({'n_estimators':20, \n",
    "          'max_depth':100, \n",
    "          'min_samples_split':2,\n",
    "          'min_samples_leaf': 1,\n",
    "          'max_features': 130},\n",
    "        lazy = False)\n",
    "\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that the Random Forest algorithm doesn't do as well as simple regularised linear models. It may be able to perform better with a different tuning protocol, perhaps something to explore in another notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating optimiser for xgboost, note that doing it this way isn't entirely necessary\n",
    "#xgboost already contains the inbuilt functionality to do so, but I want to be consistent in my approach\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgb_score(eta, max_depth, gamma, colsample, subsample, early_stop):\n",
    "    \n",
    "    #Contraining hyperparameters to be converted to integers (e.g. number of decision trees can't be continuous!)\n",
    "    max_depth = int(max_depth)\n",
    "    early_stop = int(early_stop)\n",
    "    \n",
    "    #assert type(n_estimators) == int\n",
    "    assert type(max_depth) == int\n",
    "    assert type(early_stop) == int\n",
    "    \n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=np.array(train_data['SalePrice'])\n",
    "    \n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(learning_rate=0.1, \n",
    "                                 max_depth=max_depth, \n",
    "                                 min_split_loss=gamma, \n",
    "                                 colsample_bytree = colsample,\n",
    "                                 subsample = subsample,\n",
    "                                 early_stopping_rounds = early_stop,\n",
    "                                 objective=\"reg:squarederror\")\n",
    "    \n",
    "    scores = cross_val_score(estimator=xgb_model, X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_xgb = {'eta': (0.01,0.5),\n",
    "              'max_depth':(3,100),\n",
    "              'gamma':(0,0.4), \n",
    "              'colsample':(0.3,1), \n",
    "              'subsample':(0.3,1),\n",
    "              'early_stop':(1,15)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsample | early_... |    eta    |   gamma   | max_depth | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1365  \u001b[0m | \u001b[0m 0.8162  \u001b[0m | \u001b[0m 2.225   \u001b[0m | \u001b[0m 0.3302  \u001b[0m | \u001b[0m 0.2041  \u001b[0m | \u001b[0m 42.16   \u001b[0m | \u001b[0m 0.9408  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.1417  \u001b[0m | \u001b[0m 0.6035  \u001b[0m | \u001b[0m 3.87    \u001b[0m | \u001b[0m 0.1764  \u001b[0m | \u001b[0m 0.3534  \u001b[0m | \u001b[0m 70.71   \u001b[0m | \u001b[0m 0.5208  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.1289  \u001b[0m | \u001b[95m 0.6892  \u001b[0m | \u001b[95m 9.789   \u001b[0m | \u001b[95m 0.01241 \u001b[0m | \u001b[95m 0.05395 \u001b[0m | \u001b[95m 87.04   \u001b[0m | \u001b[95m 0.46    \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1354  \u001b[0m | \u001b[0m 0.8753  \u001b[0m | \u001b[0m 1.18    \u001b[0m | \u001b[0m 0.4698  \u001b[0m | \u001b[0m 0.1247  \u001b[0m | \u001b[0m 32.91   \u001b[0m | \u001b[0m 0.3206  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.1326  \u001b[0m | \u001b[0m 0.6031  \u001b[0m | \u001b[0m 8.196   \u001b[0m | \u001b[0m 0.3365  \u001b[0m | \u001b[0m 0.109   \u001b[0m | \u001b[0m 63.32   \u001b[0m | \u001b[0m 0.3158  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1332  \u001b[0m | \u001b[0m 0.6579  \u001b[0m | \u001b[0m 14.94   \u001b[0m | \u001b[0m 0.1875  \u001b[0m | \u001b[0m 0.1536  \u001b[0m | \u001b[0m 3.011   \u001b[0m | \u001b[0m 0.5212  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.1369  \u001b[0m | \u001b[0m 0.7394  \u001b[0m | \u001b[0m 14.69   \u001b[0m | \u001b[0m 0.4205  \u001b[0m | \u001b[0m 0.3296  \u001b[0m | \u001b[0m 99.94   \u001b[0m | \u001b[0m 0.7298  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1341  \u001b[0m | \u001b[0m 0.3075  \u001b[0m | \u001b[0m 14.72   \u001b[0m | \u001b[0m 0.4661  \u001b[0m | \u001b[0m 0.1676  \u001b[0m | \u001b[0m 3.033   \u001b[0m | \u001b[0m 0.4105  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.1379  \u001b[0m | \u001b[0m 0.6596  \u001b[0m | \u001b[0m 1.265   \u001b[0m | \u001b[0m 0.4493  \u001b[0m | \u001b[0m 0.2338  \u001b[0m | \u001b[0m 99.97   \u001b[0m | \u001b[0m 0.484   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.1422  \u001b[0m | \u001b[0m 0.695   \u001b[0m | \u001b[0m 14.77   \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 0.2693  \u001b[0m | \u001b[0m 3.007   \u001b[0m | \u001b[0m 0.983   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.1399  \u001b[0m | \u001b[0m 0.3338  \u001b[0m | \u001b[0m 1.245   \u001b[0m | \u001b[0m 0.032   \u001b[0m | \u001b[0m 0.2636  \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 0.7578  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1302  \u001b[0m | \u001b[0m 0.5089  \u001b[0m | \u001b[0m 14.6    \u001b[0m | \u001b[0m 0.4924  \u001b[0m | \u001b[0m 0.08847 \u001b[0m | \u001b[0m 3.053   \u001b[0m | \u001b[0m 0.3373  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1377  \u001b[0m | \u001b[0m 0.7721  \u001b[0m | \u001b[0m 14.9    \u001b[0m | \u001b[0m 0.236   \u001b[0m | \u001b[0m 0.1757  \u001b[0m | \u001b[0m 3.045   \u001b[0m | \u001b[0m 0.3028  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1372  \u001b[0m | \u001b[0m 0.8239  \u001b[0m | \u001b[0m 1.052   \u001b[0m | \u001b[0m 0.4451  \u001b[0m | \u001b[0m 0.06177 \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.8796  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1354  \u001b[0m | \u001b[0m 0.3584  \u001b[0m | \u001b[0m 14.78   \u001b[0m | \u001b[0m 0.02539 \u001b[0m | \u001b[0m 0.1666  \u001b[0m | \u001b[0m 3.044   \u001b[0m | \u001b[0m 0.63    \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.1373  \u001b[0m | \u001b[0m 0.5581  \u001b[0m | \u001b[0m 1.002   \u001b[0m | \u001b[0m 0.03658 \u001b[0m | \u001b[0m 0.2294  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.5214  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.1295  \u001b[0m | \u001b[0m 0.8461  \u001b[0m | \u001b[0m 14.9    \u001b[0m | \u001b[0m 0.2204  \u001b[0m | \u001b[0m 0.09604 \u001b[0m | \u001b[0m 3.007   \u001b[0m | \u001b[0m 0.828   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1434  \u001b[0m | \u001b[0m 0.8647  \u001b[0m | \u001b[0m 14.65   \u001b[0m | \u001b[0m 0.3047  \u001b[0m | \u001b[0m 0.3471  \u001b[0m | \u001b[0m 3.028   \u001b[0m | \u001b[0m 0.3125  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.1411  \u001b[0m | \u001b[0m 0.8424  \u001b[0m | \u001b[0m 14.54   \u001b[0m | \u001b[0m 0.09638 \u001b[0m | \u001b[0m 0.3637  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.8369  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1332  \u001b[0m | \u001b[0m 0.8971  \u001b[0m | \u001b[0m 1.293   \u001b[0m | \u001b[0m 0.272   \u001b[0m | \u001b[0m 0.06498 \u001b[0m | \u001b[0m 3.02    \u001b[0m | \u001b[0m 0.356   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1387  \u001b[0m | \u001b[0m 0.954   \u001b[0m | \u001b[0m 1.08    \u001b[0m | \u001b[0m 0.08307 \u001b[0m | \u001b[0m 0.3212  \u001b[0m | \u001b[0m 3.041   \u001b[0m | \u001b[0m 0.3924  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1351  \u001b[0m | \u001b[0m 0.604   \u001b[0m | \u001b[0m 14.91   \u001b[0m | \u001b[0m 0.06316 \u001b[0m | \u001b[0m 0.2294  \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 0.5326  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1338  \u001b[0m | \u001b[0m 0.4874  \u001b[0m | \u001b[0m 1.115   \u001b[0m | \u001b[0m 0.1472  \u001b[0m | \u001b[0m 0.189   \u001b[0m | \u001b[0m 3.042   \u001b[0m | \u001b[0m 0.9013  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1349  \u001b[0m | \u001b[0m 0.4828  \u001b[0m | \u001b[0m 1.18    \u001b[0m | \u001b[0m 0.4614  \u001b[0m | \u001b[0m 0.177   \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 0.4464  \u001b[0m |\n",
      "| \u001b[95m 25      \u001b[0m | \u001b[95m-0.1287  \u001b[0m | \u001b[95m 0.7311  \u001b[0m | \u001b[95m 1.126   \u001b[0m | \u001b[95m 0.06281 \u001b[0m | \u001b[95m 0.06639 \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 0.7838  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.1433  \u001b[0m | \u001b[0m 0.4958  \u001b[0m | \u001b[0m 1.215   \u001b[0m | \u001b[0m 0.4145  \u001b[0m | \u001b[0m 0.3924  \u001b[0m | \u001b[0m 3.004   \u001b[0m | \u001b[0m 0.4394  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1369  \u001b[0m | \u001b[0m 0.4517  \u001b[0m | \u001b[0m 14.51   \u001b[0m | \u001b[0m 0.4049  \u001b[0m | \u001b[0m 0.291   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.9616  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.1403  \u001b[0m | \u001b[0m 0.3583  \u001b[0m | \u001b[0m 14.66   \u001b[0m | \u001b[0m 0.2879  \u001b[0m | \u001b[0m 0.3219  \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 0.9865  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.1406  \u001b[0m | \u001b[0m 0.7803  \u001b[0m | \u001b[0m 1.018   \u001b[0m | \u001b[0m 0.1044  \u001b[0m | \u001b[0m 0.3226  \u001b[0m | \u001b[0m 3.012   \u001b[0m | \u001b[0m 0.8069  \u001b[0m |\n",
      "| \u001b[95m 30      \u001b[0m | \u001b[95m-0.126   \u001b[0m | \u001b[95m 0.3274  \u001b[0m | \u001b[95m 14.77   \u001b[0m | \u001b[95m 0.2338  \u001b[0m | \u001b[95m 7.62e-05\u001b[0m | \u001b[95m 3.017   \u001b[0m | \u001b[95m 0.4897  \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1346  \u001b[0m | \u001b[0m 0.6956  \u001b[0m | \u001b[0m 14.88   \u001b[0m | \u001b[0m 0.4433  \u001b[0m | \u001b[0m 0.1179  \u001b[0m | \u001b[0m 3.042   \u001b[0m | \u001b[0m 0.5217  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.1373  \u001b[0m | \u001b[0m 0.7584  \u001b[0m | \u001b[0m 14.98   \u001b[0m | \u001b[0m 0.1985  \u001b[0m | \u001b[0m 0.2568  \u001b[0m | \u001b[0m 3.027   \u001b[0m | \u001b[0m 0.9794  \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1451  \u001b[0m | \u001b[0m 0.4513  \u001b[0m | \u001b[0m 14.84   \u001b[0m | \u001b[0m 0.3614  \u001b[0m | \u001b[0m 0.3922  \u001b[0m | \u001b[0m 99.95   \u001b[0m | \u001b[0m 0.9358  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1336  \u001b[0m | \u001b[0m 0.8085  \u001b[0m | \u001b[0m 1.029   \u001b[0m | \u001b[0m 0.2874  \u001b[0m | \u001b[0m 0.01064 \u001b[0m | \u001b[0m 3.016   \u001b[0m | \u001b[0m 0.8112  \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.1415  \u001b[0m | \u001b[0m 0.3841  \u001b[0m | \u001b[0m 1.001   \u001b[0m | \u001b[0m 0.283   \u001b[0m | \u001b[0m 0.2964  \u001b[0m | \u001b[0m 3.069   \u001b[0m | \u001b[0m 0.4491  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1375  \u001b[0m | \u001b[0m 0.3511  \u001b[0m | \u001b[0m 1.172   \u001b[0m | \u001b[0m 0.1636  \u001b[0m | \u001b[0m 0.2216  \u001b[0m | \u001b[0m 3.001   \u001b[0m | \u001b[0m 0.9363  \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.1292  \u001b[0m | \u001b[0m 0.7486  \u001b[0m | \u001b[0m 14.98   \u001b[0m | \u001b[0m 0.2476  \u001b[0m | \u001b[0m 0.05953 \u001b[0m | \u001b[0m 99.87   \u001b[0m | \u001b[0m 0.556   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.1402  \u001b[0m | \u001b[0m 0.4133  \u001b[0m | \u001b[0m 14.92   \u001b[0m | \u001b[0m 0.4652  \u001b[0m | \u001b[0m 0.353   \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 0.8672  \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1426  \u001b[0m | \u001b[0m 0.8625  \u001b[0m | \u001b[0m 14.8    \u001b[0m | \u001b[0m 0.2291  \u001b[0m | \u001b[0m 0.3208  \u001b[0m | \u001b[0m 3.022   \u001b[0m | \u001b[0m 0.3588  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.1339  \u001b[0m | \u001b[0m 0.6914  \u001b[0m | \u001b[0m 1.002   \u001b[0m | \u001b[0m 0.08829 \u001b[0m | \u001b[0m 0.1574  \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 0.8849  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.1402  \u001b[0m | \u001b[0m 0.4192  \u001b[0m | \u001b[0m 14.83   \u001b[0m | \u001b[0m 0.2535  \u001b[0m | \u001b[0m 0.2671  \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 0.5233  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1352  \u001b[0m | \u001b[0m 0.9423  \u001b[0m | \u001b[0m 1.043   \u001b[0m | \u001b[0m 0.3573  \u001b[0m | \u001b[0m 0.1335  \u001b[0m | \u001b[0m 3.03    \u001b[0m | \u001b[0m 0.9395  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.1346  \u001b[0m | \u001b[0m 0.5615  \u001b[0m | \u001b[0m 1.512   \u001b[0m | \u001b[0m 0.1691  \u001b[0m | \u001b[0m 0.153   \u001b[0m | \u001b[0m 3.002   \u001b[0m | \u001b[0m 0.4532  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.1326  \u001b[0m | \u001b[0m 0.9385  \u001b[0m | \u001b[0m 1.229   \u001b[0m | \u001b[0m 0.182   \u001b[0m | \u001b[0m 0.1009  \u001b[0m | \u001b[0m 3.039   \u001b[0m | \u001b[0m 0.9928  \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.1342  \u001b[0m | \u001b[0m 0.3772  \u001b[0m | \u001b[0m 1.012   \u001b[0m | \u001b[0m 0.4222  \u001b[0m | \u001b[0m 0.1522  \u001b[0m | \u001b[0m 3.024   \u001b[0m | \u001b[0m 0.7066  \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.1425  \u001b[0m | \u001b[0m 0.8767  \u001b[0m | \u001b[0m 1.261   \u001b[0m | \u001b[0m 0.469   \u001b[0m | \u001b[0m 0.2939  \u001b[0m | \u001b[0m 99.93   \u001b[0m | \u001b[0m 0.5871  \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1323  \u001b[0m | \u001b[0m 0.4848  \u001b[0m | \u001b[0m 14.96   \u001b[0m | \u001b[0m 0.2111  \u001b[0m | \u001b[0m 0.1382  \u001b[0m | \u001b[0m 3.057   \u001b[0m | \u001b[0m 0.8584  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1409  \u001b[0m | \u001b[0m 0.6836  \u001b[0m | \u001b[0m 14.91   \u001b[0m | \u001b[0m 0.03267 \u001b[0m | \u001b[0m 0.3843  \u001b[0m | \u001b[0m 3.06    \u001b[0m | \u001b[0m 0.7896  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.1372  \u001b[0m | \u001b[0m 0.7637  \u001b[0m | \u001b[0m 1.05    \u001b[0m | \u001b[0m 0.3595  \u001b[0m | \u001b[0m 0.1869  \u001b[0m | \u001b[0m 3.015   \u001b[0m | \u001b[0m 0.3219  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.139   \u001b[0m | \u001b[0m 0.9292  \u001b[0m | \u001b[0m 14.87   \u001b[0m | \u001b[0m 0.04694 \u001b[0m | \u001b[0m 0.3497  \u001b[0m | \u001b[0m 3.038   \u001b[0m | \u001b[0m 0.6527  \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.1305  \u001b[0m | \u001b[0m 0.6795  \u001b[0m | \u001b[0m 1.166   \u001b[0m | \u001b[0m 0.2706  \u001b[0m | \u001b[0m 0.0393  \u001b[0m | \u001b[0m 99.94   \u001b[0m | \u001b[0m 0.4343  \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.1375  \u001b[0m | \u001b[0m 0.7095  \u001b[0m | \u001b[0m 1.196   \u001b[0m | \u001b[0m 0.01438 \u001b[0m | \u001b[0m 0.1784  \u001b[0m | \u001b[0m 99.99   \u001b[0m | \u001b[0m 0.7967  \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.1407  \u001b[0m | \u001b[0m 0.8773  \u001b[0m | \u001b[0m 1.155   \u001b[0m | \u001b[0m 0.2436  \u001b[0m | \u001b[0m 0.3108  \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 0.4383  \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.1361  \u001b[0m | \u001b[0m 0.5804  \u001b[0m | \u001b[0m 14.95   \u001b[0m | \u001b[0m 0.387   \u001b[0m | \u001b[0m 0.2161  \u001b[0m | \u001b[0m 3.028   \u001b[0m | \u001b[0m 0.8475  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.1305  \u001b[0m | \u001b[0m 0.4683  \u001b[0m | \u001b[0m 14.78   \u001b[0m | \u001b[0m 0.3857  \u001b[0m | \u001b[0m 0.08117 \u001b[0m | \u001b[0m 3.049   \u001b[0m | \u001b[0m 0.4479  \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune Random Forests regression\n",
    "\n",
    "BO = BayesianOptimization(xgb_score, bounds_xgb)\n",
    "\n",
    "BO.probe({'eta': 0.1,\n",
    "              'max_depth':40,\n",
    "              'gamma':0.05, \n",
    "              'colsample':0.9, \n",
    "              'subsample':0.7,\n",
    "              'early_stop':10},\n",
    "         lazy=False)\n",
    "     \n",
    "BO.maximize(n_iter=50,alpha=0.0001)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost has the capacity to perform just as well as the Ridge regression and the Lasso. Let's do afinal check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.12624005832880553 -0.12838112190136214 -0.14018589593028044 -0.12966348849718665\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lasso_score(0.001),\n",
    "    ridge_score(10),\n",
    "    RF_score(n_estimators=300, \n",
    "                    max_depth=100, \n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf = 2,\n",
    "                    max_features = 130),\n",
    "    xgb_score(eta=0.2338,\n",
    "              max_depth=3,\n",
    "              gamma=0,\n",
    "              colsample=0.3,\n",
    "              subsample=0.7,\n",
    "              early_stop=15)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso, Ridge regression and xgboost seem to have very similar performance. Perhaps we can stack the predictions together? This might work if they are not very strongly correlated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha=0.001)\n",
    "model_lasso.fit(X_train,SalePrice)\n",
    "lasso_preds = pd.Series(np.expm1(model_lasso.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = Ridge(alpha=10)\n",
    "model_lasso.fit(X_train,SalePrice)\n",
    "ridge_preds = pd.Series(np.expm1(model_lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRegressor(learning_rate=0.2338,\n",
    "                             max_depth=3, \n",
    "                             min_split_loss=0,\n",
    "                             colsample=0.3,\n",
    "                             subsample=0.7,\n",
    "                             objective=\"reg:squarederror\")\n",
    "model_xgb.fit(X_train,SalePrice)\n",
    "xgb_preds = pd.Series(np.expm1(model_xgb.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2\n",
      "0  1.000000  0.995993  0.958713\n",
      "1  0.995993  1.000000  0.961008\n",
      "2  0.958713  0.961008  1.000000\n"
     ]
    }
   ],
   "source": [
    "preds = pd.concat([lasso_preds, ridge_preds, xgb_preds], axis=1)\n",
    "corrs = preds.corr()\n",
    "print(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the lasso and the ridge have almost idential predictions, xgboost offers diversification while providing equivalent performance, let's try blending them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     119615.256807\n",
       "1     146555.611916\n",
       "2     174436.241111\n",
       "3     193700.379548\n",
       "4     193803.680011\n",
       "5     170172.231215\n",
       "6     180437.401231\n",
       "7     160312.391393\n",
       "8     190902.977971\n",
       "9     119963.531807\n",
       "10    188804.136908\n",
       "11     97270.278039\n",
       "12     96265.048406\n",
       "13    147538.571270\n",
       "14    114757.636177\n",
       "15    347984.592648\n",
       "16    238691.156341\n",
       "17    288202.386870\n",
       "18    277584.559951\n",
       "19    461782.904861\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_final = preds.mean(axis=1)\n",
    "preds_final.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "submission['SalePrice'] = preds_final\n",
    "submission.to_csv('Data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A public score of 0.12481, not too shabby. Though using the the Lasso predictions by themselves would have netted me a score of 0.12357, and placed me in the top 30%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
