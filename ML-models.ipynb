{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will assess a variety of ML algorithms in their performance in predicting house prices. We'll be considering lasso and ridge regression, random forests, xgboost and maybe later light gbm. All models will be tuned via Bayesian Optimisation to minimise the average 5-fold cross validation score. Note that here specifically we are predicting log house prices and using the RMSE metric so our model evaluation will need to be tailored to that.\n",
    "\n",
    "First we'll import the data and clean it so that it can be used in a ML friendly format. We'll use a similar process to the one in the EDA notebook. Since exploratory data analysis has already been covered in the other notebook, we won't bother repeating it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import skew\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train.csv')\n",
    "test = pd.read_csv('Data/test.csv')\n",
    "\n",
    "#Stripping SalePrice from the training data and combining with the test data\n",
    "SalePrice = np.log(train['SalePrice'])\n",
    "train = train.drop('SalePrice',axis=1)\n",
    "X = pd.concat([train,test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSSubClass is a categorical variable, so we convert it to a string \n",
    "X['MSSubClass'] = X['MSSubClass'].apply(str)\n",
    "X['LotFrontage'] = X['LotFrontage'].fillna(0)\n",
    "train['GarageYrBlt'] = train['GarageYrBlt'].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dealing with numerics\n",
    "numerics = X.select_dtypes(exclude='object')\n",
    "\n",
    "#Filling nas\n",
    "numerics = numerics.fillna(method='ffill')\n",
    "\n",
    "#log transformation of skewed variables\n",
    "skews = numerics.apply(lambda x:skew(x.dropna()))\n",
    "skewed = skews>0.75\n",
    "skewed_data = numerics[skewed.index[skewed]]\n",
    "skewed_feats = skewed_data.columns\n",
    "numerics = numerics.drop(skewed_feats, axis=1)\n",
    "log_transformed = np.log1p(skewed_data)\n",
    "numerics = pd.concat([numerics,log_transformed],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 304) (1459, 304)\n",
      "(1460, 305) (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "#Dealing with strings\n",
    "strings = X.select_dtypes(include='object')\n",
    "\n",
    "#Converting strings to dummies and joining with numerics\n",
    "dummies = pd.get_dummies(strings)\n",
    "X = pd.concat([numerics,dummies],axis=1)\n",
    "\n",
    "#Splitting into train and test data\n",
    "X_train = X.iloc[:train.shape[0],]\n",
    "X_test = X.iloc[train.shape[0]:,]\n",
    "train = pd.concat([SalePrice,X_train],axis=1)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that the initial data preprocessing has been done, we are now going to set up cross-validation and Bayesian optimisation procedures for each model. The reason why we use Bayesian Optimisation for hyperparameter tuning is because of the stochastic nature of our objective function. We are randomly sorting the data to better replicate the variability in the dgp when measuring the validation score. While a deterministic optimiser may work, it may not be the best solution if it does not account for the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up optimisation for Ridge regression model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_score(alpha):\n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "    \n",
    "    scores = cross_val_score(estimator=Ridge(alpha=alpha), X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_ridge = {'alpha': (0,20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the parameter bounds, I have chosen alpha to be between 0 and 20. Using a 0 value will be equivalent to estimation by OLS, any positive values will shrink the parameter estimates towards zero. I do not want to using an upper bound that is too high as it will waste computation power as in practice, optimal values for alpha are relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1329  \u001b[0m | \u001b[0m 63.23   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.1284  \u001b[0m | \u001b[95m 13.79   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.133   \u001b[0m | \u001b[0m 75.6    \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1352  \u001b[0m | \u001b[0m 95.52   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 155.2   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1407  \u001b[0m | \u001b[0m 200.0   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-1.658e+1\u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1297  \u001b[0m | \u001b[0m 11.57   \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m-0.1282  \u001b[0m | \u001b[95m 10.57   \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m-0.1257  \u001b[0m | \u001b[95m 13.01   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.1266  \u001b[0m | \u001b[0m 13.47   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1285  \u001b[0m | \u001b[0m 10.24   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1259  \u001b[0m | \u001b[0m 13.62   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1268  \u001b[0m | \u001b[0m 10.17   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1281  \u001b[0m | \u001b[0m 13.63   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.1302  \u001b[0m | \u001b[0m 10.16   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.1281  \u001b[0m | \u001b[0m 13.63   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1293  \u001b[0m | \u001b[0m 10.15   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.1313  \u001b[0m | \u001b[0m 11.1    \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1277  \u001b[0m | \u001b[0m 12.64   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1301  \u001b[0m | \u001b[0m 10.15   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1293  \u001b[0m | \u001b[0m 13.64   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1309  \u001b[0m | \u001b[0m 10.15   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1266  \u001b[0m | \u001b[0m 13.64   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1287  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.1276  \u001b[0m | \u001b[0m 13.64   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1292  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[95m 28      \u001b[0m | \u001b[95m-0.1254  \u001b[0m | \u001b[95m 13.64   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.129   \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.1262  \u001b[0m | \u001b[0m 13.64   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1302  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.1292  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1312  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1279  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.1279  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1284  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.1269  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.1278  \u001b[0m | \u001b[0m 10.87   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1266  \u001b[0m | \u001b[0m 12.81   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.1273  \u001b[0m | \u001b[0m 10.14   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.1281  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1269  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.1294  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.129   \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.1311  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.126   \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1298  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1286  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.1271  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.1308  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[95m 51      \u001b[0m | \u001b[95m-0.1254  \u001b[0m | \u001b[95m 10.13   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.1287  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.1296  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.1288  \u001b[0m | \u001b[0m 13.65   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.1265  \u001b[0m | \u001b[0m 10.13   \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune ridge regression\n",
    "BO = BayesianOptimization(ridge_score, bounds_ridge)\n",
    "\n",
    "BO.probe({\"alpha\":10},lazy=False)\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal choice of alpha seems to be around 10-20. Though depending on how the optimiser progresses, it can sometimes get stuck near the upper bound. I had to use the probe method to ensure that the optimizer started at a good value, otherwise it can get stuck on the upperbound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up optimisation for Lasso model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def lasso_score(alpha):\n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "\n",
    "    scores = cross_val_score(estimator=Lasso(alpha=alpha, max_iter=10000), X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_lasso = {'alpha': (0.00001,20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adjusting the bounds for the lasso as 0 values for regularisation cause convergence problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.3077  \u001b[0m | \u001b[0m 10.5    \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.3068  \u001b[0m | \u001b[0m 11.0    \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.3131  \u001b[0m | \u001b[0m 16.5    \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.3041  \u001b[0m | \u001b[0m 4.121   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.3163  \u001b[0m | \u001b[0m 19.36   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1382  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.14    \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1407  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.137   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.137   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.1405  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1385  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1387  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1387  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1387  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.1358  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.139   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1356  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1325  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1394  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.1394  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1431  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.1431  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1343  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1343  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.1343  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1364  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.1364  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.1368  \u001b[0m | \u001b[0m 1.002e-0\u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1384  \u001b[0m | \u001b[0m 1.002e-0\u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.1357  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.1427  \u001b[0m | \u001b[0m 1.003e-0\u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1416  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.1363  \u001b[0m | \u001b[0m 1.003e-0\u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.1363  \u001b[0m | \u001b[0m 1.003e-0\u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.1363  \u001b[0m | \u001b[0m 1.003e-0\u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.1416  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1416  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1374  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.143   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.143   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.143   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.1397  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.1397  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.139   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.139   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune Lasso regression\n",
    "\n",
    "BO = BayesianOptimization(lasso_score, bounds_lasso)\n",
    "\n",
    "BO.probe({\"alpha\":0.001},lazy=False)\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm quite suspicious about the behaviour of the Bayes optimiser in this case, it cannot seem to explore other values than the lower bound. While I could change the lower bound, it will cause convergence issues and will require an increased number of iterations. I can handpick a value for alpha that does noticeably better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12852462107663287"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_score(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I'm not going to try delve into the workings of optimiser as I don't think it will be very fruitful. Instead I'm going to try using a different optimiser to see if we get better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.26752613266792663\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-213127.13829123])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 50\n",
       "      nit: 2\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.99999921])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(lasso_score, x0=1, bounds = ((0.00001,20),), tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also performs quite poorly. I think it gets thrown off by the randomness of the objective function. It does find a good optimum, but only if I put in an optimal starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.1276295819828786\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([89516.17478705])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 36\n",
       "      nit: 2\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.00099648])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(lasso_score, x0=0.001, bounds = ((0.00001,20),), tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ridge and lasso seem to perform quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating optimizer for a Random Forests regressor\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "\n",
    "def RF_score(n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features):\n",
    "    \n",
    "    #Contraining hyperparameters to be converted to integers (e.g. number of decision trees can't be continuous!)\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    min_samples_split = int(min_samples_split)\n",
    "    min_samples_leaf = int(min_samples_leaf)\n",
    "    max_features = int(max_features)\n",
    "    \n",
    "    assert type(n_estimators) == int\n",
    "    assert type(max_depth) == int\n",
    "    assert type(min_samples_split) == int\n",
    "    assert type(min_samples_leaf) == int\n",
    "    assert type(max_features) == int\n",
    "    \n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        estimator=RF(\n",
    "                    n_estimators=n_estimators, \n",
    "                    max_depth=max_depth, \n",
    "                    min_samples_split=min_samples_split,\n",
    "                    min_samples_leaf = min_samples_leaf,\n",
    "                    max_features = max_features),\n",
    "    X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_RF = {\n",
    "    'n_estimators': (1,3000),\n",
    "    'max_depth': (1,100),\n",
    "    'min_samples_split': (2,200),\n",
    "    'min_samples_leaf': (1,200),\n",
    "    'max_features': (1,290)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | max_fe... | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1793  \u001b[0m | \u001b[0m 6.65    \u001b[0m | \u001b[0m 278.8   \u001b[0m | \u001b[0m 13.95   \u001b[0m | \u001b[0m 99.06   \u001b[0m | \u001b[0m 1.956e+0\u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.2507  \u001b[0m | \u001b[0m 27.67   \u001b[0m | \u001b[0m 219.0   \u001b[0m | \u001b[0m 165.7   \u001b[0m | \u001b[0m 41.0    \u001b[0m | \u001b[0m 1.803e+0\u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.1568  \u001b[0m | \u001b[0m 91.48   \u001b[0m | \u001b[0m 112.6   \u001b[0m | \u001b[0m 10.95   \u001b[0m | \u001b[0m 48.4    \u001b[0m | \u001b[0m 1.858e+0\u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1627  \u001b[0m | \u001b[0m 14.49   \u001b[0m | \u001b[0m 124.5   \u001b[0m | \u001b[0m 2.196   \u001b[0m | \u001b[0m 71.42   \u001b[0m | \u001b[0m 2.041e+0\u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.1947  \u001b[0m | \u001b[0m 78.26   \u001b[0m | \u001b[0m 95.5    \u001b[0m | \u001b[0m 70.24   \u001b[0m | \u001b[0m 148.3   \u001b[0m | \u001b[0m 2.381e+0\u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.2829  \u001b[0m | \u001b[0m 63.24   \u001b[0m | \u001b[0m 2.142   \u001b[0m | \u001b[0m 3.485   \u001b[0m | \u001b[0m 192.6   \u001b[0m | \u001b[0m 804.0   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.148   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 3e+03   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1658  \u001b[0m | \u001b[0m 22.58   \u001b[0m | \u001b[0m 10.69   \u001b[0m | \u001b[0m 3.329   \u001b[0m | \u001b[0m 10.52   \u001b[0m | \u001b[0m 2.974e+0\u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.1721  \u001b[0m | \u001b[0m 81.14   \u001b[0m | \u001b[0m 287.1   \u001b[0m | \u001b[0m 11.62   \u001b[0m | \u001b[0m 68.91   \u001b[0m | \u001b[0m 8.593   \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m-0.1418  \u001b[0m | \u001b[95m 79.81   \u001b[0m | \u001b[95m 185.9   \u001b[0m | \u001b[95m 2.233   \u001b[0m | \u001b[95m 5.928   \u001b[0m | \u001b[95m 2.422e+0\u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.2199  \u001b[0m | \u001b[0m 7.049   \u001b[0m | \u001b[0m 35.72   \u001b[0m | \u001b[0m 15.66   \u001b[0m | \u001b[0m 5.282   \u001b[0m | \u001b[0m 1.318   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1495  \u001b[0m | \u001b[0m 89.6    \u001b[0m | \u001b[0m 288.7   \u001b[0m | \u001b[0m 7.301   \u001b[0m | \u001b[0m 5.742   \u001b[0m | \u001b[0m 709.8   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1421  \u001b[0m | \u001b[0m 97.0    \u001b[0m | \u001b[0m 278.2   \u001b[0m | \u001b[0m 2.444   \u001b[0m | \u001b[0m 8.099   \u001b[0m | \u001b[0m 1.67e+03\u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1443  \u001b[0m | \u001b[0m 87.46   \u001b[0m | \u001b[0m 289.4   \u001b[0m | \u001b[0m 2.253   \u001b[0m | \u001b[0m 4.19    \u001b[0m | \u001b[0m 2.443e+0\u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1514  \u001b[0m | \u001b[0m 94.32   \u001b[0m | \u001b[0m 285.2   \u001b[0m | \u001b[0m 10.58   \u001b[0m | \u001b[0m 4.383   \u001b[0m | \u001b[0m 198.0   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.2062  \u001b[0m | \u001b[0m 96.84   \u001b[0m | \u001b[0m 24.86   \u001b[0m | \u001b[0m 1.642   \u001b[0m | \u001b[0m 183.8   \u001b[0m | \u001b[0m 2.961e+0\u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.1423  \u001b[0m | \u001b[0m 97.5    \u001b[0m | \u001b[0m 108.6   \u001b[0m | \u001b[0m 4.631   \u001b[0m | \u001b[0m 6.116   \u001b[0m | \u001b[0m 2.334e+0\u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1458  \u001b[0m | \u001b[0m 99.77   \u001b[0m | \u001b[0m 26.36   \u001b[0m | \u001b[0m 2.727   \u001b[0m | \u001b[0m 7.354   \u001b[0m | \u001b[0m 2.156e+0\u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.1466  \u001b[0m | \u001b[0m 90.44   \u001b[0m | \u001b[0m 281.8   \u001b[0m | \u001b[0m 3.693   \u001b[0m | \u001b[0m 6.894   \u001b[0m | \u001b[0m 2.058e+0\u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1502  \u001b[0m | \u001b[0m 84.55   \u001b[0m | \u001b[0m 289.6   \u001b[0m | \u001b[0m 3.046   \u001b[0m | \u001b[0m 6.503   \u001b[0m | \u001b[0m 23.8    \u001b[0m |\n",
      "| \u001b[95m 21      \u001b[0m | \u001b[95m-0.1394  \u001b[0m | \u001b[95m 97.13   \u001b[0m | \u001b[95m 243.4   \u001b[0m | \u001b[95m 2.385   \u001b[0m | \u001b[95m 2.36    \u001b[0m | \u001b[95m 1.861e+0\u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1452  \u001b[0m | \u001b[0m 11.83   \u001b[0m | \u001b[0m 241.7   \u001b[0m | \u001b[0m 1.976   \u001b[0m | \u001b[0m 13.65   \u001b[0m | \u001b[0m 2.577e+0\u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1482  \u001b[0m | \u001b[0m 98.68   \u001b[0m | \u001b[0m 277.1   \u001b[0m | \u001b[0m 5.01    \u001b[0m | \u001b[0m 9.813   \u001b[0m | \u001b[0m 2.054e+0\u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1608  \u001b[0m | \u001b[0m 92.76   \u001b[0m | \u001b[0m 9.78    \u001b[0m | \u001b[0m 2.162   \u001b[0m | \u001b[0m 5.616   \u001b[0m | \u001b[0m 2.296e+0\u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1437  \u001b[0m | \u001b[0m 86.7    \u001b[0m | \u001b[0m 270.0   \u001b[0m | \u001b[0m 1.942   \u001b[0m | \u001b[0m 3.788   \u001b[0m | \u001b[0m 1.459e+0\u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.1447  \u001b[0m | \u001b[0m 52.52   \u001b[0m | \u001b[0m 279.8   \u001b[0m | \u001b[0m 3.224   \u001b[0m | \u001b[0m 3.544   \u001b[0m | \u001b[0m 2.426e+0\u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1478  \u001b[0m | \u001b[0m 18.79   \u001b[0m | \u001b[0m 285.9   \u001b[0m | \u001b[0m 4.152   \u001b[0m | \u001b[0m 8.59    \u001b[0m | \u001b[0m 2.939e+0\u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.2875  \u001b[0m | \u001b[0m 1.47    \u001b[0m | \u001b[0m 281.0   \u001b[0m | \u001b[0m 4.546   \u001b[0m | \u001b[0m 3.22    \u001b[0m | \u001b[0m 2.36e+03\u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.2859  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 224.4   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune Rando Forests regression\n",
    "BO = BayesianOptimization(RF_score, bounds_RF)\n",
    "\n",
    "BO.probe({'n_estimators':20, \n",
    "          'max_depth':100, \n",
    "          'min_samples_split':2,\n",
    "          'min_samples_leaf': 1,\n",
    "          'max_features': 130},\n",
    "        lazy = False)\n",
    "\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that the Random Forest algorithm doesn't do as well as simple regularised linear models. It may be able to perform better with a different tuning protocol, perhaps something to explore in another notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating optimiser for xgboost, note that doing it this way isn't entirely necessary\n",
    "#xgboost already contains the inbuilt functionality to do so, but I want to be consistent in my approach\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgb_score(eta, max_depth, gamma, colsample, subsample, early_stop):\n",
    "    \n",
    "    #Contraining hyperparameters to be converted to integers (e.g. number of decision trees can't be continuous!)\n",
    "    max_depth = int(max_depth)\n",
    "    early_stop = int(early_stop)\n",
    "    \n",
    "    #assert type(n_estimators) == int\n",
    "    assert type(max_depth) == int\n",
    "    assert type(early_stop) == int\n",
    "    \n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=np.array(train_data['SalePrice'])\n",
    "    \n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(learning_rate=0.1, \n",
    "                                 max_depth=max_depth, \n",
    "                                 min_split_loss=gamma, \n",
    "                                 colsample_bytree = colsample,\n",
    "                                 subsample = subsample,\n",
    "                                 early_stopping_rounds = early_stop,\n",
    "                                 objective=\"reg:squarederror\")\n",
    "    \n",
    "    scores = cross_val_score(estimator=xgb_model, X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_xgb = {'eta': (0.01,0.5),\n",
    "              'max_depth':(3,100),\n",
    "              'gamma':(0,0.4), \n",
    "              'colsample':(0.3,1), \n",
    "              'subsample':(0.3,1),\n",
    "              'early_stop':(1,15)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsample | early_... |    eta    |   gamma   | max_depth | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1421  \u001b[0m | \u001b[0m 0.9163  \u001b[0m | \u001b[0m 8.453   \u001b[0m | \u001b[0m 0.07675 \u001b[0m | \u001b[0m 0.3335  \u001b[0m | \u001b[0m 19.24   \u001b[0m | \u001b[0m 0.9383  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.1381  \u001b[0m | \u001b[95m 0.4751  \u001b[0m | \u001b[95m 11.29   \u001b[0m | \u001b[95m 0.118   \u001b[0m | \u001b[95m 0.2687  \u001b[0m | \u001b[95m 11.04   \u001b[0m | \u001b[95m 0.7378  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.1326  \u001b[0m | \u001b[95m 0.7861  \u001b[0m | \u001b[95m 13.81   \u001b[0m | \u001b[95m 0.08371 \u001b[0m | \u001b[95m 0.07473 \u001b[0m | \u001b[95m 21.65   \u001b[0m | \u001b[95m 0.8461  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-0.1305  \u001b[0m | \u001b[95m 0.7299  \u001b[0m | \u001b[95m 9.31    \u001b[0m | \u001b[95m 0.4099  \u001b[0m | \u001b[95m 0.1261  \u001b[0m | \u001b[95m 27.06   \u001b[0m | \u001b[95m 0.3844  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.1388  \u001b[0m | \u001b[0m 0.3472  \u001b[0m | \u001b[0m 1.883   \u001b[0m | \u001b[0m 0.03803 \u001b[0m | \u001b[0m 0.2748  \u001b[0m | \u001b[0m 15.2    \u001b[0m | \u001b[0m 0.9554  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1369  \u001b[0m | \u001b[0m 0.5111  \u001b[0m | \u001b[0m 1.219   \u001b[0m | \u001b[0m 0.2971  \u001b[0m | \u001b[0m 0.002935\u001b[0m | \u001b[0m 39.98   \u001b[0m | \u001b[0m 0.3715  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.1398  \u001b[0m | \u001b[0m 0.3249  \u001b[0m | \u001b[0m 15.0    \u001b[0m | \u001b[0m 0.3365  \u001b[0m | \u001b[0m 0.3099  \u001b[0m | \u001b[0m 39.91   \u001b[0m | \u001b[0m 0.4689  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1309  \u001b[0m | \u001b[0m 0.9567  \u001b[0m | \u001b[0m 1.13    \u001b[0m | \u001b[0m 0.2592  \u001b[0m | \u001b[0m 0.02828 \u001b[0m | \u001b[0m 3.003   \u001b[0m | \u001b[0m 0.7279  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.1337  \u001b[0m | \u001b[0m 0.6189  \u001b[0m | \u001b[0m 1.046   \u001b[0m | \u001b[0m 0.2366  \u001b[0m | \u001b[0m 0.08828 \u001b[0m | \u001b[0m 3.002   \u001b[0m | \u001b[0m 0.4639  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.1425  \u001b[0m | \u001b[0m 0.6641  \u001b[0m | \u001b[0m 14.76   \u001b[0m | \u001b[0m 0.4694  \u001b[0m | \u001b[0m 0.2594  \u001b[0m | \u001b[0m 3.012   \u001b[0m | \u001b[0m 0.3292  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.1381  \u001b[0m | \u001b[0m 0.5835  \u001b[0m | \u001b[0m 1.037   \u001b[0m | \u001b[0m 0.4114  \u001b[0m | \u001b[0m 0.1929  \u001b[0m | \u001b[0m 39.95   \u001b[0m | \u001b[0m 0.7457  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1406  \u001b[0m | \u001b[0m 0.5551  \u001b[0m | \u001b[0m 1.106   \u001b[0m | \u001b[0m 0.3686  \u001b[0m | \u001b[0m 0.3612  \u001b[0m | \u001b[0m 40.0    \u001b[0m | \u001b[0m 0.4581  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1353  \u001b[0m | \u001b[0m 0.3469  \u001b[0m | \u001b[0m 14.95   \u001b[0m | \u001b[0m 0.3582  \u001b[0m | \u001b[0m 0.2051  \u001b[0m | \u001b[0m 3.03    \u001b[0m | \u001b[0m 0.5816  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1391  \u001b[0m | \u001b[0m 0.3998  \u001b[0m | \u001b[0m 14.9    \u001b[0m | \u001b[0m 0.4874  \u001b[0m | \u001b[0m 0.3455  \u001b[0m | \u001b[0m 3.01    \u001b[0m | \u001b[0m 0.5654  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1328  \u001b[0m | \u001b[0m 0.7921  \u001b[0m | \u001b[0m 1.039   \u001b[0m | \u001b[0m 0.3033  \u001b[0m | \u001b[0m 0.03781 \u001b[0m | \u001b[0m 39.98   \u001b[0m | \u001b[0m 0.3526  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.132   \u001b[0m | \u001b[0m 0.9368  \u001b[0m | \u001b[0m 1.025   \u001b[0m | \u001b[0m 0.03736 \u001b[0m | \u001b[0m 0.04424 \u001b[0m | \u001b[0m 39.94   \u001b[0m | \u001b[0m 0.3343  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.1384  \u001b[0m | \u001b[0m 0.5048  \u001b[0m | \u001b[0m 1.028   \u001b[0m | \u001b[0m 0.17    \u001b[0m | \u001b[0m 0.2697  \u001b[0m | \u001b[0m 39.99   \u001b[0m | \u001b[0m 0.9216  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1332  \u001b[0m | \u001b[0m 0.7636  \u001b[0m | \u001b[0m 14.85   \u001b[0m | \u001b[0m 0.2068  \u001b[0m | \u001b[0m 0.1042  \u001b[0m | \u001b[0m 3.006   \u001b[0m | \u001b[0m 0.9574  \u001b[0m |\n",
      "| \u001b[95m 19      \u001b[0m | \u001b[95m-0.129   \u001b[0m | \u001b[95m 0.5977  \u001b[0m | \u001b[95m 14.96   \u001b[0m | \u001b[95m 0.1622  \u001b[0m | \u001b[95m 0.000487\u001b[0m | \u001b[95m 3.041   \u001b[0m | \u001b[95m 0.6669  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1326  \u001b[0m | \u001b[0m 0.5514  \u001b[0m | \u001b[0m 14.96   \u001b[0m | \u001b[0m 0.1227  \u001b[0m | \u001b[0m 0.04177 \u001b[0m | \u001b[0m 3.079   \u001b[0m | \u001b[0m 0.8157  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1333  \u001b[0m | \u001b[0m 0.4639  \u001b[0m | \u001b[0m 15.0    \u001b[0m | \u001b[0m 0.0989  \u001b[0m | \u001b[0m 0.1601  \u001b[0m | \u001b[0m 3.019   \u001b[0m | \u001b[0m 0.3443  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1341  \u001b[0m | \u001b[0m 0.3115  \u001b[0m | \u001b[0m 1.117   \u001b[0m | \u001b[0m 0.248   \u001b[0m | \u001b[0m 0.2526  \u001b[0m | \u001b[0m 3.016   \u001b[0m | \u001b[0m 0.9844  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1297  \u001b[0m | \u001b[0m 0.4343  \u001b[0m | \u001b[0m 14.99   \u001b[0m | \u001b[0m 0.3718  \u001b[0m | \u001b[0m 0.1153  \u001b[0m | \u001b[0m 39.95   \u001b[0m | \u001b[0m 0.4056  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1383  \u001b[0m | \u001b[0m 0.5182  \u001b[0m | \u001b[0m 14.95   \u001b[0m | \u001b[0m 0.1694  \u001b[0m | \u001b[0m 0.2044  \u001b[0m | \u001b[0m 39.99   \u001b[0m | \u001b[0m 0.9092  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1351  \u001b[0m | \u001b[0m 0.9479  \u001b[0m | \u001b[0m 1.064   \u001b[0m | \u001b[0m 0.2458  \u001b[0m | \u001b[0m 0.228   \u001b[0m | \u001b[0m 3.006   \u001b[0m | \u001b[0m 0.7444  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.1335  \u001b[0m | \u001b[0m 0.762   \u001b[0m | \u001b[0m 14.98   \u001b[0m | \u001b[0m 0.3499  \u001b[0m | \u001b[0m 0.005964\u001b[0m | \u001b[0m 40.0    \u001b[0m | \u001b[0m 0.6048  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1299  \u001b[0m | \u001b[0m 0.8491  \u001b[0m | \u001b[0m 1.022   \u001b[0m | \u001b[0m 0.3372  \u001b[0m | \u001b[0m 0.04612 \u001b[0m | \u001b[0m 3.056   \u001b[0m | \u001b[0m 0.9479  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.1328  \u001b[0m | \u001b[0m 0.6795  \u001b[0m | \u001b[0m 1.071   \u001b[0m | \u001b[0m 0.1613  \u001b[0m | \u001b[0m 0.1146  \u001b[0m | \u001b[0m 3.03    \u001b[0m | \u001b[0m 0.8944  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.135   \u001b[0m | \u001b[0m 0.5258  \u001b[0m | \u001b[0m 1.179   \u001b[0m | \u001b[0m 0.3735  \u001b[0m | \u001b[0m 0.261   \u001b[0m | \u001b[0m 3.015   \u001b[0m | \u001b[0m 0.7036  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.1459  \u001b[0m | \u001b[0m 0.5472  \u001b[0m | \u001b[0m 14.91   \u001b[0m | \u001b[0m 0.3465  \u001b[0m | \u001b[0m 0.3992  \u001b[0m | \u001b[0m 39.96   \u001b[0m | \u001b[0m 0.484   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1313  \u001b[0m | \u001b[0m 0.3345  \u001b[0m | \u001b[0m 1.19    \u001b[0m | \u001b[0m 0.3903  \u001b[0m | \u001b[0m 0.03393 \u001b[0m | \u001b[0m 3.002   \u001b[0m | \u001b[0m 0.4929  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.1362  \u001b[0m | \u001b[0m 0.7747  \u001b[0m | \u001b[0m 1.047   \u001b[0m | \u001b[0m 0.03002 \u001b[0m | \u001b[0m 0.1948  \u001b[0m | \u001b[0m 3.027   \u001b[0m | \u001b[0m 0.4333  \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1375  \u001b[0m | \u001b[0m 0.4725  \u001b[0m | \u001b[0m 1.07    \u001b[0m | \u001b[0m 0.391   \u001b[0m | \u001b[0m 0.208   \u001b[0m | \u001b[0m 3.008   \u001b[0m | \u001b[0m 0.9154  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1368  \u001b[0m | \u001b[0m 0.5598  \u001b[0m | \u001b[0m 14.95   \u001b[0m | \u001b[0m 0.1694  \u001b[0m | \u001b[0m 0.2798  \u001b[0m | \u001b[0m 3.02    \u001b[0m | \u001b[0m 0.4242  \u001b[0m |\n",
      "| \u001b[95m 35      \u001b[0m | \u001b[95m-0.1286  \u001b[0m | \u001b[95m 0.8823  \u001b[0m | \u001b[95m 1.112   \u001b[0m | \u001b[95m 0.1288  \u001b[0m | \u001b[95m 0.04007 \u001b[0m | \u001b[95m 39.98   \u001b[0m | \u001b[95m 0.5538  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1403  \u001b[0m | \u001b[0m 0.7741  \u001b[0m | \u001b[0m 1.054   \u001b[0m | \u001b[0m 0.3346  \u001b[0m | \u001b[0m 0.2842  \u001b[0m | \u001b[0m 39.94   \u001b[0m | \u001b[0m 0.3109  \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.1425  \u001b[0m | \u001b[0m 0.8918  \u001b[0m | \u001b[0m 1.086   \u001b[0m | \u001b[0m 0.169   \u001b[0m | \u001b[0m 0.3134  \u001b[0m | \u001b[0m 3.011   \u001b[0m | \u001b[0m 0.3111  \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.1398  \u001b[0m | \u001b[0m 0.4318  \u001b[0m | \u001b[0m 14.74   \u001b[0m | \u001b[0m 0.3918  \u001b[0m | \u001b[0m 0.3949  \u001b[0m | \u001b[0m 40.0    \u001b[0m | \u001b[0m 0.4843  \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1335  \u001b[0m | \u001b[0m 0.4278  \u001b[0m | \u001b[0m 14.8    \u001b[0m | \u001b[0m 0.3127  \u001b[0m | \u001b[0m 0.1887  \u001b[0m | \u001b[0m 3.02    \u001b[0m | \u001b[0m 0.9504  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 0.9118  \u001b[0m | \u001b[0m 14.98   \u001b[0m | \u001b[0m 0.3477  \u001b[0m | \u001b[0m 0.3183  \u001b[0m | \u001b[0m 3.033   \u001b[0m | \u001b[0m 0.4727  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.1414  \u001b[0m | \u001b[0m 0.6635  \u001b[0m | \u001b[0m 1.154   \u001b[0m | \u001b[0m 0.1065  \u001b[0m | \u001b[0m 0.2877  \u001b[0m | \u001b[0m 39.93   \u001b[0m | \u001b[0m 0.3357  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1354  \u001b[0m | \u001b[0m 0.8645  \u001b[0m | \u001b[0m 1.001   \u001b[0m | \u001b[0m 0.1087  \u001b[0m | \u001b[0m 0.1388  \u001b[0m | \u001b[0m 3.01    \u001b[0m | \u001b[0m 0.9076  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.1333  \u001b[0m | \u001b[0m 0.3641  \u001b[0m | \u001b[0m 14.87   \u001b[0m | \u001b[0m 0.2673  \u001b[0m | \u001b[0m 0.0859  \u001b[0m | \u001b[0m 3.102   \u001b[0m | \u001b[0m 0.6656  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.1417  \u001b[0m | \u001b[0m 0.3968  \u001b[0m | \u001b[0m 14.93   \u001b[0m | \u001b[0m 0.3931  \u001b[0m | \u001b[0m 0.3663  \u001b[0m | \u001b[0m 3.004   \u001b[0m | \u001b[0m 0.6151  \u001b[0m |\n",
      "| \u001b[95m 45      \u001b[0m | \u001b[95m-0.1265  \u001b[0m | \u001b[95m 0.9766  \u001b[0m | \u001b[95m 1.01    \u001b[0m | \u001b[95m 0.09145 \u001b[0m | \u001b[95m 0.0302  \u001b[0m | \u001b[95m 39.97   \u001b[0m | \u001b[95m 0.9682  \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.1315  \u001b[0m | \u001b[0m 0.7125  \u001b[0m | \u001b[0m 1.03    \u001b[0m | \u001b[0m 0.2269  \u001b[0m | \u001b[0m 0.003604\u001b[0m | \u001b[0m 39.99   \u001b[0m | \u001b[0m 0.3973  \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1352  \u001b[0m | \u001b[0m 0.6523  \u001b[0m | \u001b[0m 1.011   \u001b[0m | \u001b[0m 0.2979  \u001b[0m | \u001b[0m 0.1106  \u001b[0m | \u001b[0m 39.98   \u001b[0m | \u001b[0m 0.5149  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1313  \u001b[0m | \u001b[0m 0.3491  \u001b[0m | \u001b[0m 1.049   \u001b[0m | \u001b[0m 0.4327  \u001b[0m | \u001b[0m 0.1537  \u001b[0m | \u001b[0m 39.98   \u001b[0m | \u001b[0m 0.9182  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.1347  \u001b[0m | \u001b[0m 0.5481  \u001b[0m | \u001b[0m 1.034   \u001b[0m | \u001b[0m 0.2438  \u001b[0m | \u001b[0m 0.104   \u001b[0m | \u001b[0m 39.98   \u001b[0m | \u001b[0m 0.3492  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.1406  \u001b[0m | \u001b[0m 0.9482  \u001b[0m | \u001b[0m 1.011   \u001b[0m | \u001b[0m 0.3564  \u001b[0m | \u001b[0m 0.3222  \u001b[0m | \u001b[0m 39.99   \u001b[0m | \u001b[0m 0.6083  \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.1326  \u001b[0m | \u001b[0m 0.5703  \u001b[0m | \u001b[0m 1.037   \u001b[0m | \u001b[0m 0.4504  \u001b[0m | \u001b[0m 0.1016  \u001b[0m | \u001b[0m 3.023   \u001b[0m | \u001b[0m 0.9569  \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.1291  \u001b[0m | \u001b[0m 0.504   \u001b[0m | \u001b[0m 14.95   \u001b[0m | \u001b[0m 0.3867  \u001b[0m | \u001b[0m 0.0374  \u001b[0m | \u001b[0m 3.018   \u001b[0m | \u001b[0m 0.7305  \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.1404  \u001b[0m | \u001b[0m 0.8258  \u001b[0m | \u001b[0m 14.99   \u001b[0m | \u001b[0m 0.3848  \u001b[0m | \u001b[0m 0.3104  \u001b[0m | \u001b[0m 3.009   \u001b[0m | \u001b[0m 0.4784  \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.1383  \u001b[0m | \u001b[0m 0.5643  \u001b[0m | \u001b[0m 1.075   \u001b[0m | \u001b[0m 0.2045  \u001b[0m | \u001b[0m 0.1931  \u001b[0m | \u001b[0m 39.98   \u001b[0m | \u001b[0m 0.7325  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.1428  \u001b[0m | \u001b[0m 0.5509  \u001b[0m | \u001b[0m 15.0    \u001b[0m | \u001b[0m 0.176   \u001b[0m | \u001b[0m 0.3965  \u001b[0m | \u001b[0m 3.013   \u001b[0m | \u001b[0m 0.7466  \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune Random Forests regression\n",
    "\n",
    "BO = BayesianOptimization(xgb_score, bounds_xgb)\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we've tuned the hyperparameters using Bayes opt and gotten a good feel of how the models perform using different parameter sets, it's pretty easy to see that xgboost performs the best. As a final evaluation, I'll put in some paramter values to evaluate our models concurrently and then pick from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1269655787503537 -0.12987026651616576 -0.14191146053416806 -0.1309665650518888\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lasso_score(0.001),\n",
    "    ridge_score(10),\n",
    "    RF_score(n_estimators=20, \n",
    "                    max_depth=100, \n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf = 1,\n",
    "                    max_features = 130),\n",
    "    xgb_score(eta=0.1,\n",
    "              max_depth=40,\n",
    "              gamma=0.05,\n",
    "              colsample=0.9,\n",
    "              subsample=0.7,\n",
    "              early_stop=10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, xgboost clearly outperforms all other models. So we'll use that configuration to make our test predictions and submit on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRegressor(learning_rate=0.1, max_depth=10, min_split_loss=0, objective=\"reg:squarederror\")\n",
    "model_xgb.fit(X_train,SalePrice)\n",
    "xgb_preds = np.expm1(model_xgb.predict(X_test))\n",
    "\n",
    "\n",
    "#Submission csv\n",
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "submission['SalePrice'] = xgb_preds\n",
    "submission.to_csv('Data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well! Turns out we did rather poorly despite all the work we put in, we got a public score of 0.14504 placing us in the bottom 50% :(."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha=0.001)\n",
    "model_lasso.fit(X_train,SalePrice)\n",
    "lasso_preds = np.expm1(model_lasso.predict(X_test))\n",
    "\n",
    "#Submission csv\n",
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "submission['SalePrice'] = lasso_preds\n",
    "submission.to_csv('Data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
