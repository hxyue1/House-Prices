{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will assess a variety of ML algorithms in their performance in predicting house prices. We'll be considering lasso and ridge regression, random forests, xgboost and maybe later light gbm. All models will be tuned via Bayesian Optimisation in order to minimise the average 5-fold cross validation score. Note that here specifically we are predicting log house prices and using the RMSE metric so our model evaluation will need to be tailored to that.\n",
    "\n",
    "First we'll import the data and clean it so that it can be used in a ML friendly format. We'll use a combination of random shuffling and forward filling to get rid of the NAs and convert the strings into dummy variables. We will not be exploring feature creation as there are many features in this data set already. Since exploratory data analysis has already been covered in the other notebook, we won't bother repeating it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "train = pd.read_csv('Data/train.csv')\n",
    "test = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up forward filling function\n",
    "\n",
    "def fill_nas(df):\n",
    "    \n",
    "    #Keeping tack of time\n",
    "    t0 = time.time()\n",
    "    \n",
    "    #Counting NaNs\n",
    "    na_count = df.isna().sum().sum()\n",
    "    \n",
    "    while na_count>0:\n",
    "        df = df.sample(frac=1)\n",
    "        df = df.fillna(method='ffill',limit=11)\n",
    "        na_count = df.isna().sum().sum()\n",
    "\n",
    "    filled_df = df.sort_index()\n",
    "    \n",
    "    #Calculating time taken\n",
    "    t1 = time.time()\n",
    "    print(t1-t0)\n",
    "    \n",
    "    #Return filled df\n",
    "    return(filled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13582897186279297\n",
      "0.10307097434997559\n"
     ]
    }
   ],
   "source": [
    "train = fill_nas(train)\n",
    "test = fill_nas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stripping SalePrice from the training data and combining with the test data\n",
    "SalePrice = np.log(train['SalePrice'])\n",
    "train = train.drop('SalePrice',axis=1)\n",
    "X = pd.concat([train,test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 304) (1459, 304)\n",
      "(1460, 305) (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "#Getting strings and numerics\n",
    "#MSSubClass is a categorical variable, so we convert it to a string \n",
    "X['MSSubClass'] = X['MSSubClass'].apply(str) \n",
    "numerics = X.select_dtypes(exclude='object')\n",
    "strings = X.select_dtypes(include='object')\n",
    "\n",
    "#Converting strings to dummies and joining with numerics\n",
    "dummies = pd.get_dummies(strings)\n",
    "X = pd.concat([numerics,dummies],axis=1)\n",
    "\n",
    "#Splitting into train and test data\n",
    "X_train = X.iloc[:train.shape[0],]\n",
    "X_test = X.iloc[train.shape[0]:,]\n",
    "train = pd.concat([SalePrice,X_train],axis=1)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that the initial data preprocessing has been done, we are now going to set up cross-validation and Bayesian optimisation procedures for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up optimisation for Ridge regression model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_score(alpha):\n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "\n",
    "    scores = cross_val_score(estimator=Ridge(alpha=alpha), X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_ridge = {'alpha': (0,1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up optimisation for Lasso model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def lasso_score(alpha):\n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "\n",
    "    scores = cross_val_score(estimator=Lasso(alpha=alpha), X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_lasso = {'alpha': (0.0000000000000000001,0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating optimizer for a Random Forests regressor\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "\n",
    "def RF_score(n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features):\n",
    "    \n",
    "    #Contraining hyperparameters to be converted to integers (e.g. number of decision trees can't be continuous!)\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    min_samples_split = int(min_samples_split)\n",
    "    min_samples_leaf = int(min_samples_leaf)\n",
    "    max_features = int(max_features)\n",
    "    \n",
    "    assert type(n_estimators) == int\n",
    "    assert type(max_depth) == int\n",
    "    assert type(min_samples_split) == int\n",
    "    assert type(min_samples_leaf) == int\n",
    "    assert type(max_features) == int\n",
    "    \n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        estimator=RF(\n",
    "                    n_estimators=n_estimators, \n",
    "                    max_depth=max_depth, \n",
    "                    min_samples_split=min_samples_split,\n",
    "                    min_samples_leaf = min_samples_leaf,\n",
    "                    max_features = max_features),\n",
    "    X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_RF = {\n",
    "    'n_estimators': (1,3000),\n",
    "    'max_depth': (1,100),\n",
    "    'min_samples_split': (2,200),\n",
    "    'min_samples_leaf': (1,200),\n",
    "    'max_features': (1,290)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating optimiser for xgboost, note that doing it this way isn't entirely necessary\n",
    "#xgboost already contains the inbuilt functionality to do so, but I want to be consistent in my approach\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgb_score(eta, max_depth, gamma):\n",
    "    \n",
    "    #Contraining hyperparameters to be converted to integers (e.g. number of decision trees can't be continuous!)\n",
    "    #n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    #min_samples_split = int(min_samples_split)\n",
    "    #min_samples_leaf = int(min_samples_leaf)\n",
    "    #max_features = int(max_features)\n",
    "    \n",
    "    #assert type(n_estimators) == int\n",
    "    assert type(max_depth) == int\n",
    "    #assert type(min_samples_split) == int\n",
    "    #assert type(min_samples_leaf) == int\n",
    "    #assert type(max_features) == int\n",
    "    \n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    #X = xgb.DMatrix(X)\n",
    "    y=np.array(train_data['SalePrice'])\n",
    "    \n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(learning_rate=0.1, \n",
    "                                 max_depth=max_depth, \n",
    "                                 min_split_loss=gamma, \n",
    "                                 objective=\"reg:squarederror\")\n",
    "    \n",
    "    scores = cross_val_score(estimator=xgb_model, X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_xgb = {'eta': (0.08,0.12),'max_depth':(5,15),'gamma':(0,0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing models\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "BO = BayesianOptimization(xgb_score, bounds_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   gamma   | max_depth |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1335  \u001b[0m | \u001b[0m 0.01501 \u001b[0m | \u001b[0m 9.988   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.3997  \u001b[0m | \u001b[0m 0.0909  \u001b[0m | \u001b[0m 0.9037  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.1383  \u001b[0m | \u001b[0m 0.06796 \u001b[0m | \u001b[0m 13.18   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1387  \u001b[0m | \u001b[0m 0.09692 \u001b[0m | \u001b[0m 2.963   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.1371  \u001b[0m | \u001b[0m 0.01459 \u001b[0m | \u001b[0m 11.42   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1401  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 5.981   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.131   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 15.0    \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1396  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 4.395   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.1367  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 8.018   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.1319  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.753   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.1327  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 15.0    \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.135   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 8.844   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1347  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 14.07   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1408  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 10.2    \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1327  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.956   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.1327  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.956   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.135   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 15.0    \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1368  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 7.333   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.1361  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 4.174   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1389  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 12.28   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1332  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 14.13   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1352  \u001b[0m | \u001b[0m 0.09985 \u001b[0m | \u001b[0m 15.0    \u001b[0m |\n",
      "| \u001b[95m 23      \u001b[0m | \u001b[95m-0.1304  \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 3.755   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1327  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 8.938   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1359  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 14.06   \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we've tuned the hyperparameters using Bayes opt and gotten a good feel of how the models perform using different parameter sets, it's pretty easy to see that xgboost performs the best. As a final evaluation, I'll put in some paramter values to evaluate our models concurrently and then pick from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14694637973357275 -0.15107785073146096 -0.14957741736316 -0.1317039883423656\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    lasso_score(0.00001),\n",
    "    ridge_score(0.01),\n",
    "    RF_score(n_estimators=1000, \n",
    "                    max_depth=100, \n",
    "                    min_samples_split=20,\n",
    "                    min_samples_leaf = 5,\n",
    "                    max_features = 250),\n",
    "    xgb_score(eta=0.1,max_depth=10,gamma=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, xgboost clearly outperforms all other models. So we'll use that configuration to make our test predictions and submit on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRegressor(learning_rate=0.1, max_depth=10, min_split_loss=0, objective=\"reg:squarederror\")\n",
    "model_xgb.fit(X_train,SalePrice)\n",
    "xgb_preds = np.expm1(model_xgb.predict(X_test))\n",
    "\n",
    "#Submission csv\n",
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "submission['SalePrice'] = xgb_preds\n",
    "submission.to_csv('Data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well! Turns out we did rather poorly despite all the work we put in, we got a public score of 0.14504 placing us in the bottom 50% :(."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
