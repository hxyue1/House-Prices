{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will assess a variety of ML algorithms in their performance in predicting house prices. We'll be considering lasso and ridge regression, random forests, xgboost and maybe later light gbm. All models will be tuned via Bayesian Optimisation in order to minimise the average 5-fold cross validation score. Note that here specifically we are predicting log house prices and using the RMSE metric so our model evaluation will need to be tailored to that.\n",
    "\n",
    "First we'll import the data and clean it so that it can be used in a ML friendly format. We'll use a combination of random shuffling and forward filling to get rid of the NAs and convert the strings into dummy variables. We will not be exploring feature creation as there are many features in this data set already. Since exploratory data analysis has already been covered in the other notebook, we won't bother repeating it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import skew\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up forward filling function\n",
    "\n",
    "def fill_nas(df):\n",
    "    \n",
    "    #Keeping tack of time\n",
    "    t0 = time.time()\n",
    "    \n",
    "    #Counting NaNs\n",
    "    na_count = df.isna().sum().sum()\n",
    "    \n",
    "    while na_count>0:\n",
    "        df = df.sample(frac=1)\n",
    "        df = df.fillna(method='ffill',limit=1)\n",
    "        na_count = df.isna().sum().sum()\n",
    "\n",
    "    filled_df = df.sort_index()\n",
    "    \n",
    "    #Calculating time taken\n",
    "    t1 = time.time()\n",
    "    print(t1-t0)\n",
    "    \n",
    "    #Return filled df\n",
    "    return(filled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train.csv')\n",
    "test = pd.read_csv('Data/test.csv')\n",
    "\n",
    "#Stripping SalePrice from the training data and combining with the test data\n",
    "SalePrice = np.log(train['SalePrice'])\n",
    "train = train.drop('SalePrice',axis=1)\n",
    "X = pd.concat([train,test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSSubClass is a categorical variable, so we convert it to a string \n",
    "X['MSSubClass'] = X['MSSubClass'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025222301483154297\n"
     ]
    }
   ],
   "source": [
    "##Dealing with numerics\n",
    "numerics = X.select_dtypes(exclude='object')\n",
    "\n",
    "#Filling nas\n",
    "numerics = fill_nas(numerics)\n",
    "\n",
    "#log transformation of skewed variables\n",
    "skews = numerics.apply(lambda x:skew(x.dropna()))\n",
    "skewed = skews>0.75\n",
    "skewed_data = numerics[skewed.index[skewed]]\n",
    "skewed_feats = skewed_data.columns\n",
    "numerics = numerics.drop(skewed_feats, axis=1)\n",
    "log_transformed = np.log1p(skewed_data)\n",
    "numerics = pd.concat([numerics,log_transformed],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 348) (1459, 348)\n",
      "(1460, 349) (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "#Dealing with strings\n",
    "strings = X.select_dtypes(include='object')\n",
    "\n",
    "#Converting strings to dummies and joining with numerics\n",
    "dummies = pd.get_dummies(strings, dummy_na=True)\n",
    "X = pd.concat([numerics,dummies],axis=1)\n",
    "\n",
    "#Splitting into train and test data\n",
    "X_train = X.iloc[:train.shape[0],]\n",
    "X_test = X.iloc[train.shape[0]:,]\n",
    "train = pd.concat([SalePrice,X_train],axis=1)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that the initial data preprocessing has been done, we are now going to set up cross-validation and Bayesian optimisation procedures for each model. The reason why we use Bayesian Optimisation for hyperparameter tuning is because of the stochastic nature of our objective function. We are randomly sorting the data to better replicate the variability in the dgp when measuring the validation score. While a deterministic optimiser may work, it may not be the best solution if it does not account for the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up optimisation for Ridge regression model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_score(alpha):\n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "    \n",
    "    scores = cross_val_score(estimator=Ridge(alpha=alpha), X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_ridge = {'alpha': (0,20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the parameter bounds, I have chosen alpha to be between 0 and 20. Using a 0 value will be equivalent to estimation by OLS, any positive values will shrink the parameter estimates towards zero. I do not want to using an upper bound that is too high as it will waste computation power as in practice, optimal values for alpha are relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1287  \u001b[0m | \u001b[0m 9.72    \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.1265  \u001b[0m | \u001b[95m 10.92   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.1317  \u001b[0m | \u001b[0m 14.69   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1266  \u001b[0m | \u001b[0m 7.17    \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.132   \u001b[0m | \u001b[0m 1.477   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1297  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.1516  \u001b[0m | \u001b[0m 0.001091\u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1265  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.1265  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.1288  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.1283  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1331  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1272  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1315  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1315  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.1297  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-8.803e+1\u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1283  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.1292  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1299  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1426  \u001b[0m | \u001b[0m 0.001095\u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1292  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1301  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1288  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1292  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.131   \u001b[0m | \u001b[0m 1.477   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1311  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.1301  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.1311  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.1305  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.129   \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.132   \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1285  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1279  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.1301  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1274  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.1302  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.134   \u001b[0m | \u001b[0m 1.477   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1274  \u001b[0m | \u001b[0m 7.17    \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.1298  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.1279  \u001b[0m | \u001b[0m 1.477   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1305  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.1435  \u001b[0m | \u001b[0m 0.001098\u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.1311  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.1337  \u001b[0m | \u001b[0m 1.477   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.127   \u001b[0m | \u001b[0m 10.92   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1305  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1291  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.1303  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.1293  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.1284  \u001b[0m | \u001b[0m 10.92   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.128   \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.1309  \u001b[0m | \u001b[0m 1.477   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.1281  \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.1289  \u001b[0m | \u001b[0m 10.92   \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune ridge regression\n",
    "\n",
    "BO = BayesianOptimization(ridge_score, bounds_ridge)\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal choice of alpha seems to be around 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up optimisation for Lasso model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def lasso_score(alpha):\n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "\n",
    "    scores = cross_val_score(estimator=Lasso(alpha=alpha, max_iter=10000), X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_lasso = {'alpha': (0.00001,20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adjusting the bounds for the lasso as 0 values for regularisation cause convergence problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.305   \u001b[0m | \u001b[0m 7.299   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.3066  \u001b[0m | \u001b[0m 9.549   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.3148  \u001b[0m | \u001b[0m 17.51   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-0.3044  \u001b[0m | \u001b[95m 5.857   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.316   \u001b[0m | \u001b[0m 19.1    \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-0.1356  \u001b[0m | \u001b[95m 1.006e-0\u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.1363  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-0.132   \u001b[0m | \u001b[95m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.132   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.132   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.132   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.1363  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.1418  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1394  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.1394  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.1352  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1352  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.1352  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.1396  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1396  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.136   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.136   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1338  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.1338  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.1338  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.1338  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.1338  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1397  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1366  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.1373  \u001b[0m | \u001b[0m 1.001e-0\u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.1366  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.1405  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.1405  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.1365  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.1365  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.1397  \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune Lasso regression\n",
    "\n",
    "BO = BayesianOptimization(lasso_score, bounds_lasso)\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm quite suspicious about the behaviour of the Bayes optimiser in this case, it cannot seem to explore other values than the lower bound. While I could change the lower bound, it will cause convergence issues and will require an increased number of iterations. I can handpick a value for alpha that does noticeably better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12857380432989013"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_score(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I'm not going to try delve into the workings of optimiser as I don't think it will be very fruitful. Instead I'm going to try using a different optimiser to see if we get better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.2677542767110079\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([6881.69102627])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 50\n",
       "      nit: 2\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.99999875])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(lasso_score, x0=1, bounds = ((0.00001,20),), tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also performs quite poorly. I think it gets thrown off by the randomness of the objective function. It does find a good optimum, but only if I put in an optimal starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.12544040196865947\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-196060.25283075])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 32\n",
       "      nit: 2\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.00097682])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(lasso_score, x0=0.001, bounds = ((0.00001,20),), tol=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "With a good choice of alpha, the lasso marginally outpeforms the ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating optimizer for a Random Forests regressor\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "\n",
    "def RF_score(n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features):\n",
    "    \n",
    "    #Contraining hyperparameters to be converted to integers (e.g. number of decision trees can't be continuous!)\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    min_samples_split = int(min_samples_split)\n",
    "    min_samples_leaf = int(min_samples_leaf)\n",
    "    max_features = int(max_features)\n",
    "    \n",
    "    assert type(n_estimators) == int\n",
    "    assert type(max_depth) == int\n",
    "    assert type(min_samples_split) == int\n",
    "    assert type(min_samples_leaf) == int\n",
    "    assert type(max_features) == int\n",
    "    \n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    y=train_data['SalePrice']\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        estimator=RF(\n",
    "                    n_estimators=n_estimators, \n",
    "                    max_depth=max_depth, \n",
    "                    min_samples_split=min_samples_split,\n",
    "                    min_samples_leaf = min_samples_leaf,\n",
    "                    max_features = max_features),\n",
    "    X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_RF = {\n",
    "    'n_estimators': (1,3000),\n",
    "    'max_depth': (1,100),\n",
    "    'min_samples_split': (2,200),\n",
    "    'min_samples_leaf': (1,200),\n",
    "    'max_features': (1,290)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | max_fe... | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.2525  \u001b[0m | \u001b[0m 83.77   \u001b[0m | \u001b[0m 47.77   \u001b[0m | \u001b[0m 165.1   \u001b[0m | \u001b[0m 117.2   \u001b[0m | \u001b[0m 44.25   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.2654  \u001b[0m | \u001b[0m 63.19   \u001b[0m | \u001b[0m 22.49   \u001b[0m | \u001b[0m 145.3   \u001b[0m | \u001b[0m 133.5   \u001b[0m | \u001b[0m 2.303e+0\u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.2111  \u001b[0m | \u001b[95m 65.32   \u001b[0m | \u001b[95m 236.2   \u001b[0m | \u001b[95m 107.6   \u001b[0m | \u001b[95m 143.9   \u001b[0m | \u001b[95m 946.7   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.2505  \u001b[0m | \u001b[0m 6.298   \u001b[0m | \u001b[0m 263.0   \u001b[0m | \u001b[0m 158.0   \u001b[0m | \u001b[0m 167.8   \u001b[0m | \u001b[0m 1.276e+0\u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.2573  \u001b[0m | \u001b[0m 6.909   \u001b[0m | \u001b[0m 273.4   \u001b[0m | \u001b[0m 197.2   \u001b[0m | \u001b[0m 141.0   \u001b[0m | \u001b[0m 1.167e+0\u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.2822  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 41.37   \u001b[0m | \u001b[0m 3e+03   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.1475  \u001b[0m | \u001b[95m 99.28   \u001b[0m | \u001b[95m 31.63   \u001b[0m | \u001b[95m 3.139   \u001b[0m | \u001b[95m 9.578   \u001b[0m | \u001b[95m 1.143e+0\u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.3804  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.114e+0\u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m-0.1405  \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 290.0   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 1.918e+0\u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.2284  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.3872  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 3e+03   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.2583  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.569e+0\u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.3871  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 1.573e+0\u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.141   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.262e+0\u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.2726  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.715e+0\u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.2742  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.1428  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 2.48e+03\u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.3507  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.3884  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 548.5   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.2054  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.147e+0\u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.2585  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 2.113e+0\u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.2048  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 1.205e+0\u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.1814  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 2.095e+0\u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.283   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 2.292e+0\u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.2593  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.529e+0\u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.2055  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 3e+03   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.3004  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[95m 28      \u001b[0m | \u001b[95m-0.1402  \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 290.0   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 2.0     \u001b[0m | \u001b[95m 513.6   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.2583  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 428.4   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.2574  \u001b[0m | \u001b[0m 93.44   \u001b[0m | \u001b[0m 289.8   \u001b[0m | \u001b[0m 196.0   \u001b[0m | \u001b[0m 8.438   \u001b[0m | \u001b[0m 910.4   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.1409  \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 290.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 2.815e+0\u001b[0m |\n",
      "| \u001b[95m 32      \u001b[0m | \u001b[95m-0.14    \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 138.6   \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 5.453   \u001b[0m | \u001b[95m 1.474e+0\u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.1494  \u001b[0m | \u001b[0m 96.84   \u001b[0m | \u001b[0m 106.1   \u001b[0m | \u001b[0m 9.409   \u001b[0m | \u001b[0m 3.825   \u001b[0m | \u001b[0m 2.457e+0\u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.197   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 142.4   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 624.2   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.143   \u001b[0m | \u001b[0m 97.8    \u001b[0m | \u001b[0m 140.5   \u001b[0m | \u001b[0m 4.029   \u001b[0m | \u001b[0m 7.622   \u001b[0m | \u001b[0m 294.0   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.1878  \u001b[0m | \u001b[0m 98.66   \u001b[0m | \u001b[0m 281.6   \u001b[0m | \u001b[0m 2.472   \u001b[0m | \u001b[0m 129.1   \u001b[0m | \u001b[0m 280.3   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.258   \u001b[0m | \u001b[0m 91.34   \u001b[0m | \u001b[0m 286.2   \u001b[0m | \u001b[0m 199.1   \u001b[0m | \u001b[0m 6.016   \u001b[0m | \u001b[0m 2.997e+0\u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.14    \u001b[0m | \u001b[0m 97.13   \u001b[0m | \u001b[0m 233.5   \u001b[0m | \u001b[0m 1.137   \u001b[0m | \u001b[0m 7.655   \u001b[0m | \u001b[0m 911.0   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.1671  \u001b[0m | \u001b[0m 99.92   \u001b[0m | \u001b[0m 131.6   \u001b[0m | \u001b[0m 29.78   \u001b[0m | \u001b[0m 44.94   \u001b[0m | \u001b[0m 1.182e+0\u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.399   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 200.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.2028  \u001b[0m | \u001b[0m 95.08   \u001b[0m | \u001b[0m 277.7   \u001b[0m | \u001b[0m 5.086   \u001b[0m | \u001b[0m 198.8   \u001b[0m | \u001b[0m 1.661e+0\u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.1569  \u001b[0m | \u001b[0m 6.457   \u001b[0m | \u001b[0m 228.0   \u001b[0m | \u001b[0m 14.58   \u001b[0m | \u001b[0m 3.458   \u001b[0m | \u001b[0m 317.9   \u001b[0m |\n",
      "| \u001b[95m 43      \u001b[0m | \u001b[95m-0.1386  \u001b[0m | \u001b[95m 99.84   \u001b[0m | \u001b[95m 81.03   \u001b[0m | \u001b[95m 1.853   \u001b[0m | \u001b[95m 3.489   \u001b[0m | \u001b[95m 1.736e+0\u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.1546  \u001b[0m | \u001b[0m 99.46   \u001b[0m | \u001b[0m 255.3   \u001b[0m | \u001b[0m 14.54   \u001b[0m | \u001b[0m 13.02   \u001b[0m | \u001b[0m 1.705e+0\u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.1759  \u001b[0m | \u001b[0m 98.54   \u001b[0m | \u001b[0m 279.6   \u001b[0m | \u001b[0m 12.34   \u001b[0m | \u001b[0m 95.06   \u001b[0m | \u001b[0m 2.66e+03\u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.2728  \u001b[0m | \u001b[0m 1.802   \u001b[0m | \u001b[0m 139.1   \u001b[0m | \u001b[0m 143.7   \u001b[0m | \u001b[0m 196.5   \u001b[0m | \u001b[0m 217.9   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.1483  \u001b[0m | \u001b[0m 85.61   \u001b[0m | \u001b[0m 272.7   \u001b[0m | \u001b[0m 7.917   \u001b[0m | \u001b[0m 2.936   \u001b[0m | \u001b[0m 288.0   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.1416  \u001b[0m | \u001b[0m 97.48   \u001b[0m | \u001b[0m 108.2   \u001b[0m | \u001b[0m 2.205   \u001b[0m | \u001b[0m 4.692   \u001b[0m | \u001b[0m 682.5   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.3112  \u001b[0m | \u001b[0m 93.84   \u001b[0m | \u001b[0m 12.71   \u001b[0m | \u001b[0m 198.0   \u001b[0m | \u001b[0m 2.738   \u001b[0m | \u001b[0m 2.446e+0\u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.2452  \u001b[0m | \u001b[0m 97.3    \u001b[0m | \u001b[0m 4.203   \u001b[0m | \u001b[0m 1.046   \u001b[0m | \u001b[0m 194.5   \u001b[0m | \u001b[0m 1.977e+0\u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.1537  \u001b[0m | \u001b[0m 99.2    \u001b[0m | \u001b[0m 186.0   \u001b[0m | \u001b[0m 5.002   \u001b[0m | \u001b[0m 38.71   \u001b[0m | \u001b[0m 2.23e+03\u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.1521  \u001b[0m | \u001b[0m 99.6    \u001b[0m | \u001b[0m 260.6   \u001b[0m | \u001b[0m 1.828   \u001b[0m | \u001b[0m 36.57   \u001b[0m | \u001b[0m 764.7   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.14    \u001b[0m | \u001b[0m 94.88   \u001b[0m | \u001b[0m 211.5   \u001b[0m | \u001b[0m 1.569   \u001b[0m | \u001b[0m 10.47   \u001b[0m | \u001b[0m 2.99e+03\u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.1487  \u001b[0m | \u001b[0m 96.78   \u001b[0m | \u001b[0m 156.3   \u001b[0m | \u001b[0m 9.582   \u001b[0m | \u001b[0m 19.45   \u001b[0m | \u001b[0m 470.3   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.2577  \u001b[0m | \u001b[0m 94.13   \u001b[0m | \u001b[0m 285.3   \u001b[0m | \u001b[0m 184.4   \u001b[0m | \u001b[0m 193.8   \u001b[0m | \u001b[0m 1.302e+0\u001b[0m |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune Rando Forests regression\n",
    "\n",
    "BO = BayesianOptimization(RF_score, bounds_RF)\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating optimiser for xgboost, note that doing it this way isn't entirely necessary\n",
    "#xgboost already contains the inbuilt functionality to do so, but I want to be consistent in my approach\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgb_score(eta, max_depth, gamma):\n",
    "    \n",
    "    #Contraining hyperparameters to be converted to integers (e.g. number of decision trees can't be continuous!)\n",
    "    #n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    #min_samples_split = int(min_samples_split)\n",
    "    #min_samples_leaf = int(min_samples_leaf)\n",
    "    #max_features = int(max_features)\n",
    "    \n",
    "    #assert type(n_estimators) == int\n",
    "    assert type(max_depth) == int\n",
    "    #assert type(min_samples_split) == int\n",
    "    #assert type(min_samples_leaf) == int\n",
    "    #assert type(max_features) == int\n",
    "    \n",
    "    train_data = train.sample(frac=1)\n",
    "    X=train_data.iloc[:,1:]\n",
    "    #X = xgb.DMatrix(X)\n",
    "    y=np.array(train_data['SalePrice'])\n",
    "    \n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(learning_rate=0.1, \n",
    "                                 max_depth=max_depth, \n",
    "                                 min_split_loss=gamma, \n",
    "                                 objective=\"reg:squarederror\")\n",
    "    \n",
    "    scores = cross_val_score(estimator=xgb_model, X=X, y=y, scoring='neg_mean_squared_error', cv=5)\n",
    "    return(-np.average(np.sqrt(-scores)))\n",
    "\n",
    "bounds_xgb = {'eta': (0.08,0.12),'max_depth':(5,15),'gamma':(0,0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-39e4118d69bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Running Bayes Opt to tune Randof Forests regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mBO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound_xgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb_score' is not defined"
     ]
    }
   ],
   "source": [
    "#Running Bayes Opt to tune Randof Forests regression\n",
    "\n",
    "BO = BayesianOptimization(xgb_score, bound_xgb)\n",
    "BO.maximize(n_iter=50,alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we've tuned the hyperparameters using Bayes opt and gotten a good feel of how the models perform using different parameter sets, it's pretty easy to see that xgboost performs the best. As a final evaluation, I'll put in some paramter values to evaluate our models concurrently and then pick from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14174324638541208\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    #lasso_score(0.001),\n",
    "    #ridge_score(10),\n",
    "    RF_score(n_estimators=20, \n",
    "                    max_depth=100, \n",
    "                    min_samples_split=2,\n",
    "                    min_samples_leaf = 1,\n",
    "                    max_features = 130)\n",
    "    #xgb_score(eta=0.1,max_depth=10,gamma=0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, xgboost clearly outperforms all other models. So we'll use that configuration to make our test predictions and submit on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.XGBRegressor(learning_rate=0.1, max_depth=10, min_split_loss=0, objective=\"reg:squarederror\")\n",
    "model_xgb.fit(X_train,SalePrice)\n",
    "xgb_preds = np.expm1(model_xgb.predict(X_test))\n",
    "\n",
    "\n",
    "#Submission csv\n",
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "submission['SalePrice'] = xgb_preds\n",
    "submission.to_csv('Data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well! Turns out we did rather poorly despite all the work we put in, we got a public score of 0.14504 placing us in the bottom 50% :(."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha=0.001)\n",
    "model_lasso.fit(X_train,SalePrice)\n",
    "lasso_preds = np.expm1(model_lasso.predict(X_test))\n",
    "\n",
    "#Submission csv\n",
    "submission = pd.read_csv('Data/sample_submission.csv')\n",
    "submission['SalePrice'] = lasso_preds\n",
    "submission.to_csv('Data/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
